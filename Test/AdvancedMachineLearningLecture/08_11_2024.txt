Lecture today will be on a particular kind of architecture, which I call the record and our methods. The last part of the lecture last 15 minutes, there will be the first presentation of one of the projects. The next projects will be presented at the beginning of Tuesday lecture. So make sure to bring your laptop. So there should be five in total. So let's say one today and four on Tuesday. So we start today with the last part. Okay. Great. So, yeah, I mean, just this is just another list of things that you can mention in terms of application like translating a language or image captioning so you have an image and then you will need to provide a sequence as output sequence of words. And yeah we will see some of the examples later on. So in general want to understand how to manage this kind of information with deep networks. So here are just simple examples like time series related to stock exchange biological measurements, climate measurements and so on. But of course, you know, as well as videos also have speech and music they have these time access that we should consider while operating the data. So this is the typical setting. But we might also have, as we said sequences as output so like an image Captioning we mentioned so one image as input, but then we want a sequence ofWords output. So that's it. Okay, so do you know the kind of Architecture we have been looking at has this typical structure right so we have an input we have some additional layers then towards the end we have a fully connected part. And then we have one single output right so the label like this image contains a cup and this image containing a dog and soon and so forth.
The idea of managing sequences is not only related to specific application that involve that really involved sequences. But we might also decide to adopt a sequence approach, even in settings or in probably in theory, they don't have anything on the sequential nature. So in this case, rather than an example we are looking at in this image, the logic is rather than notating an image, I want to progressively generate some part of the image, so I do it in an autoregressive way. So every time I put a piece in the image and I fill it in with the content that is required for the problem. And here we have examples both in black and white and colorful images, so this is MNIST, this is the SVHN database, street view, house numbers, they are just the numbers that you see in front of the houses, they come from Google images, right. But you can also do some crazy stuff, like input this logic into an oil paint simulator, and you have that at every step you predict what is the next kind of brush stroke that you want to input, so it can be depending on the direction, depending on color, so in subsequent steps, let's say you create this kind of oil paint, you can train a model to do that in a sequential way. Now, the core structure that we are using to do this is called recurrent neural network. And the logic. is that it, this core block is able to take the input X, but it also produces an output that then it retakes towards the output.
Long short term memory was first presented in 1997. It's a combination of linear and all linear functions. It elaborates on something produces an output that will be the hidden state. And then at the next step, it will take the next input so maybe the next word or the next character, and it will retake as input also what has been produced in the previous set. So the logic is this. So before we had simply, let's say the input X, and then some function and then the output right. So now instead we have the input XT and the output XT is just the input HD. And the output HD is thehidden state that the previous let's says step. Then we have this function, which is nothing more than I mean the W. So we are rather. reusing the same function. These states now as input by the previous by the new element of the sequence. So that is all was already an evolution of this block that must probably is I mean it's earlier is another 80 or beginning of the 90s. And you see that we're using time age. So here so originally the the recorder network. I mean they are quite old. I think that then we will see towards the end that there is a particular architecture which is called long shortterm memory that has been present in 1997 and it's called W. And we are progressively learning always the same W so the lot W is always thesame the set of parameters isAlways the same.
The idea is that we would have an initial part that works as an encoder of all the information in a sequence. And then we have another part of the network that we work as a decoder. And so, yeah, so this is again what we were looking at before so the matrix is this one. When we deal with a many too many structure in the case in which we don't have control on the length of each of the sequence right neither the input nor the output. So we learn one w that encodes the information and another w that relates into the generation of the new sequence. So, let's see in practice, what's happening during training. And let's suppose to have so character generation so typical sequence we want to produce a word, and we do it one character at a time. And for each character, we have one hot back to so like we define a vocabulary for instance now we are supposing that our vocabulary is very short is composed only of four characters. So in this case, the old will be will have a one in the last position, and the new will be in the first position. And we know also that the loss that we calculate should back propagate over all the network both involving an update for the encoder part for this w for the decoder part and for the encode and decode part. So this is what we have up here. That's what we are looking at. We want to train a model to learn out this text.
In this way, a recurrent network are let's say that the grandfather of the GPT in say GPT. We start from our input which is our very first letter, I mean it's the first character, H. And we have a hidden layer. So we elaborate on the input and through the nonlinearity we produce some value H. These will become then the input for the second element of the sequence. So this is what happens at training time so we are generating we are training a model to predict the correct next character. And then you predict the output, and then the output will become the input of the subsequent step. So towards the end of this elaborating sequences has been then it evolved in time. So if here during training you have the ground truth. So you verify that you're predicting something correctly but then the next input anyway comes. So in a test time you are generating. So it means that you have an input again you start from an agent you asked your network what should be the next character, so you go through your model that and now speaks the w all the parameters of your model is what you've learned before. And so I mean the logic or GPT right I mean we have the child or child of the child of GPT, so we will see that the logic has been evolved. So at the end    I mean   we have these and these here. So the next year together with this one. And it will have. a high probability of producing the E right.
The next step to get to GPT is self attention so it's another step but we'll try to get there in stages. So, um, now, practically you see that there is something strange in here in a sense that we are using a vocabulary composed of one of vectors. So what you're doing by considering the multiplication of every rule for these vector is just ending up selecting the first column right. So this operation here, dealing with one hot vectors it seemed, I mean, it seems trivial it seems that it might take too much time with respect to what we really want. And that's not exactly we don't want to spend time on this simple operation so what happened is that later on in terms of elaboration of the architecture itself and how to optimize the network. We passed from thinking about starting from one hot vector to starting from something which is already an embedding representation of the vectors. And then we keep it, let's say, we just at this point, the only part of the information the memory part that we have to keep is only the state here. So at  st st that summarizes what has been done because we have hidden hidden layers. So if I manage will start discussing about attention. And    the lecture if I can manage will be about attention and then the next step is to transform and to get into the next stage is GPT which is about self attention. So the lecture will be called GPT: Attention and Self Attention.
The code for an RNN in Python can be written in 112 lines. We can train a recurrent network over the whole production of sonnets from Shakespeare, and then ask the network to start generating tests so that resemble what Shakespeare would have written. And then you see that actually it produces something that resemble math, right so with the, you have the lemmas you have proof. And it has learned that he has to put this nice square at the end of the demonstration so typical things that you see, which are recurrent in a math textbook right so when you have this correlation it is able to capture the structure of the text. So, of course, if we think about just the architecture can be quite memory expensive, regardless of all the tricks that we were discussing before. But in general, this is what you can do so this is just to show you some some application we take the whole text and then one character at a time, we ask our network to produce new stuff. And at the beginning, it will be completely random. It will start creating like I mean random characters. But then I mean, through the training,. it will start filling in generating something that might resemble at least my sound like Shakespeare and these again, we are talking about RNN. So this is really the grandfather of GPT so it's that that was already a very good in its ability of producing reasonable text. This is, but these were just tests that people was doing with this kind of architecture.
The network was able to create the structure for the the C code. It also learned that he has to put the copyright note at the beginning of every piece of code. So it captured the basic knowledge that was there. And the same also with the Linux kernel, if you want, just coding, right, and you look at what is able to generate. And again, it captured some structure. So, yeah, you see, it has some meaning so every element of the values that I'm producing, they are capturing a specific kind of behavior in the text, or for instance there is another one that is tracking the length of the line so it's changed color. And here, of course, I mean, I'm supposing to be in a quite control condition which I have that every line breaks after I don't know a certain number of characters. So in that case during training it learned that so so unit in the coding, it just captures this kind of information. And this other one, when we are training, it turns, let's say, on an positive if we are inside an quote, it becomes positive when it's at an end. So I'm inside the text inside the quote, and then when I get outside it becomes a positive. And it will be a value between zero minus one one right. So we can try to visualize these values. Like a color, like saying okay so it’s a red if I'm close to one and if it's blue if I’m close to minus one.
So, we have been discussing about text because it's a typical thing that you, I mean in which you have sequence, right sequence of characters sequence of words. But then, yeah, we can reuse this kind of logic, even considering a visual input. Think about image captioning where you have an image is input and want to generate a text that describe the image. So you have your input image, the input image passes through a CNN, you can think about an image net pre trained network. You already have trained your models a pre trained model on the match net. It has been already pre trained, it's frozen. You just put this image as input to the image net, and you extract features. The image net network as a feature extractor. So this is just transfer learning because we are transferring knowledge. We are reusing this information, and then we tune only and we learn only the remaining part which is is a recurrent network that is fun so we are combining two parts of an network, a frozen part, and the part that instead we learn, but they are two different kinds of network. So,Yeah, that's time what happens, well we have an images. We have an architecture we provided as input this is a pre-trained model is a frozen network. We remove the last part right we remove the part of the FC model. And then this becomes, let's say that the first character enters in the first sentence, these enters to our network.
The team created a network that can be used to create captioning for images. They used a technique called word sampling. It is a way of predicting a word based on a distribution of probability. The network can then create a caption based on the word it has been predicted by the network. The team is now working on a version of the system that could be used for captioning in the future. They hope to release the first version of their system in the next few months. The project is being funded by the OpenAI foundation and the University of California, San Diego, and the National Institute of Standards and Technology (NIST) The team also hopes to use the technology to improve the accuracy of image captioning. They are currently working on an open source version of this system that they hope will be released in the near future. It will be available for free on the NIST website and the open-source version of NIST will be made available in the coming months. For more information on the project, visit: http://www.opensimplicit.org/network/convolution.html#network-convolution-v2.0-conviction-v1-convition-v3-convconvconv.html. For a more detailed description of the algorithm, see: http:www.openexplicit.com/convconv/v2-convv.html/v1.0/convv-conv4.html?v3.0_convv=convconv, title=convolution, keywords: "convconv"
The model was trained on data data image data in which people were not used to have a phone close to a computer. The database was containing a lot of people with surfboards close to water. Every time the network then detect somehow understand that in the image there is a person there is water, then it expects a surfboards but it's not always a case. And so, yeah, I mean, this is the typical kind of errors that then this kind of model can do. But then, the forward pass seems quite straightforward. What happens in the backward pass. So the issue is that we have to calculate the gradient of the loss with respect to ht and then pass it through the network. And then we have the next hidden layer. And this is something that autograd practically does it for you but internally when you calculate the derivative depending on the kind of function you are dealing with, you are inverting the matrix. So even if we think that every single block is just calculating or inverting a matrix, then since the w is always the same, and we are redoing this inversion multiple time, and then we Have multiple blocks like remultiplying these inverting matrix per se, in itself multiple times. And if you think about a value just a scholar, you multiply it by itself multiple multiple times and then it explodes after a while right so you multiply the number to 10,000. And the other way around if you have a value which is higher than one, which is around if     in paper, just to evaluate how good a caption is.
If you have a value which is, I mean, lower than one, then it becomes smaller and smaller and more time you multiply it by itself. So this is means that at a certain point you're not going to not learn anything anymore your gradient are not helping you to understand how to update the value of the matrix stop you. So one thing that we can do is to clip the gradient. This is a, let's say a very dirty, it's a handcrafted solution because it means that then you are not using the real gradient anymore. But this is just to save the optimization process. So what should we do? Well, we should do something that is changing the logic from the vanilla, the Cornell network, which is what we have been looking at till now, to something which is called long short term memory LSEM. So we have for operation here so rather than having so you see here G is just turn HW and it is exactly this one. So the G is doing what we expected the RNM to do. But then we have these other elements where we have a sigmoid function so different kinds of nonlinearity over this input. So, let me see what, yeah. At the end, we're not doing anything too complicated. So it's just a matter of adding some lines the code that takes care of that. But if you have punishing gradients, then that is an issue, because you really need to change the architecture to be able to manage that.
The LSEM is decomposing the network in parts and we have this sub component. So, we have the gate gate, right, which is just what we expected from an R&N. And this gate gate is multiplied with the I, okay. And then, how much I want to maintain the G information that was produced by the time age through WM this this part. So it's just sort of, you can think of it about a sort of risk. So this is the input gate so how much information I will re input for for CT and then instead this is called again the forget gate. It learns how much it is a sigmoid. It's a value between zero and one and tells me how much knowledge that is coming from the previous cell state. So you can read this F and I as rescaling the value coming from G and then you sum it up with the F that has rescaled the C T minus one, the combined together you have the next cell state, and so on. And somehow these are remembers us about what happens also with the resident we discussed last time so having this shortcuts, right to connect some some sub part of the block of the network that even in that case, we said it facilitates the flow of the gradient inside the network. And so a help us will have even longer I mean deeper network with lots of blocks. And yeah that was the, let's say that the creation of very, very deep networks. With this structure of an LST you can have a secret source of information.
Leonardo Irad: RNNs are able to manage sequences. He says they are quite flexible in the kind of architecture we want to create the vanilla RNN works. But when you deal with very long sequences then they require lots of memory. And so there are some alternative solution. I think we try to expose somehow the understanding so the explain what every elements of the network, what are we really learning inside the network. Okay, so I would say that for today we stop here for the lecture. Don't start leaving because we have the first project invitation and let's see. Leonardo Irad is one of the teaching assistants for both courses of non-smartial learning and data analysis and artificial intelligence courses. And I'm going to introduce you to the next speaker for the final examination. Leonardo: This project is mainly focused on pre-trained models as you may already know there are very big, large pre- trained models that are very used pervasively nowadays. So, general tasks like for example, summarization, rewriting bad sentences, and also due to their extensive pre-training may offer also a wealth of reusable knowledge. But the problem is that they do not have any guarantee when dealing with the domain specific knowledge. So what does it mean to personalize a language model in this case? To take it,Pre-trained, and then you find a unit on your task to suit your task specific needs. And of course this leads to huge computationalsavings.
The quest here is to see if there is some way to extract this knowledge and combine it in an efficient way. And how to do so via the so-called model editing, which is a technique. The idea is to take actually to use the task arithmetic framework that starts from task vectors which are basically just a difference between some pre-trained parameters and the parameters obtained through fine tuning starting from theta zero. And then just simply summing the parameters so you don't have to do any post-processing like other training and stuff. You may generate a model that, a multitask model that does it all better in some cases. Or instead you can suppress some unwanted behaviors. So for example, you have your model that is very, very toxic, like in this case I reported just an example. What you can do is to obtain a test vector that encodes the toxicity of the model and then by subtracting you remove the capability of being toxic. But of course in the project, this is very broad and so we will just focus on trying to build these multitask models. And here I also report some numbers just to show you that this thing actually works because you can see that the second row here reports an accuracy that is far higher than just simply using the pre- trained model. But the thing is that of course we don't know really why just Simply summing parameters may work. And so the goal of the project is to actually query and try to understand something a bit more about this task arithmeticframework.
The Kulbeck-Leibler divergence is the log determinant of a matrix. This is the so-called Fisher information matrix, which is computed as the output using the outputs and the gradients of the outputs. So it relates basically how the loss changes with how you perturb your parameters. So if you recall previously that we want to produce suitable perturbations, then this is the mathematical tool that you can use to do so in a closed form. So, yeah, I guess this concludes the presentation so thank you so much and I'll see you at the final part of the board up and on to the presentation. Thanks. Thanks you. Thank you. thank you.Thank you.    Thank you for joining us today at the MIT Sloan School of Management, where we are working on a project to build multitask models. The project is focused on collecting the results, and then present and in a comprehensive manner to try to understand what's actually going on. We will also have weekly meetings so we can discuss and actually go more into the details of these things, but yeah, no strict mathematical formalism will be required at the exam. Yeah, so just to give you a brief idea, some things are already so here you will find all the PDF that you could actually read the roughly and everything of this short presentation is hopefully summarized in a good way, but if you decide to do this project, you will. also find the what actually is already implemented in the code repository and how to use them.
The total operation is completed in: 1 minutes and 38.02 seconds