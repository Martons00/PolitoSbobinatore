So, in practice, besides, yeah, we discuss about sequences and generating let's say sentences out of an image so we discuss the case of assigning to an image, more than one single word. So, standard classification is just one input one word right so one label by discussing with about our and then we saw the example of having a sentence, which is ceiling possibility. So what we want to discuss today is something that allow the model to localize the information in different ways. So we might we will start actually from what we call object detection, which consists in finding a bounding box for every single object in the image. And then we will discuss about semantic segmentation and instant segmentation, where you will see that this is practically a combination of semantic segmentations and object detection. And so we can now we can start. Please confirm in the chapter. So. Okay, great. So before starting, you have seen probably that the, I'm going on adding papers in the spreadsheet for the assignments. And for people who asked me about moving something, I did it. Last time I remember that there was a girl who asked him to be moved from December 10. So please, I mean, the girl if he's here then afterwards during the break just come to the desk so that we can adjust the assignment because it seems that there's someone else who's missing so maybe you can do some some some movement. If she's online, just send me a message somehow we try to fix that.
There are two different ways of exploiting knowledge, let's say over multiple tasks. One is symmetric because in trans in multitask learning, what do you expect is that all the tasks help the other. Instead of transfer learning, it's asymmetric because you first learn something and then you inherit it from something else. Now again to provide you an example of this logic of machine learning, going back to the roots of the shallow machine learning approaches is just to write the position of the bounding box so the position. Now, these are standard labels as we have been discussing to now in previous lecture. The bounding boxes are identified by four numbers. So the position is usually the, either the center of thebounding box or the top left corner. So X and Y is the X andY pixel, let’s say the position inside the matrix of your image so the tensor it's RGB but still in localization in terms of height and width. And then you have how with the width and the height of the Bounding box. So right so the dimension of theBounding Box. So, now, before entering into the exact kind of strategy to solve the action. We tried a bit to abstract,  to bring this problem to a higher level so we said we need to solve two problems at the same time right so finding what and finding where. And we saw it with ImageNet right. So you train on something. And once you have trained for that task, then you reuse the knowledge for another task.
So, let's go back to standard empirical risk minimization, right? So what is the objective of empiricalrisk minimization? Is that we have some data? We've studied the correlation among this thing. In this data, we want to define a model that minimizes a certain loss. And as we said, as usual, we tend to put a regularizer, right, together with the loss. So, if we formalize the problem of multitask learning in this way, you can think about this matrix W as collecting one vector for every task. And so you have this big W here, which is in practice putting all the W in column in a matrix. And the regularizer is the Frobenius norm is just minimizing. So it's just summing up all the L2 norm of every single vector. Now, imposing the trace norm in practice says that I want a low rank solution, which means that this matrix has some of the roles, which are full of zeros. So I want this matrix to be sparse in this sense. Okay, and I have to choose which roles to set to zero, and they have to be good for all the, all the tasks. And I want to minimize in practice the dimensionality of W, so by putting a zero in practice we are forcing the collaboration among the tasks to agree on which dimensionality that is relevant. And again, it's a point that we are asking the old point on which task is not relevant and they can be removed.
Multitask, multitask solution makes sense and is helpful when every single task has a limited amount of samples, because then you are. biasing the model to prefer representation that also the other tasks prefer. What do you expect also is that since this task at the end of this training process in practically are helping each other. You might also get a sort of generalization effect improve the generalization of the model. So probably if I then have to solve the n plus one task that is similar to the previous one will have enough knowledge to do that so I generalize well. So it's a bit as a regularization because anyway I mean, it regularize in the example that we discussed here it was very clear that we were explicitly introducing the realization term, but it might not always be so evident I mean like I'm just saying by hand an extra term in general the effect that you have, regardless of the way which will implement the multitask approach it's that you can get a bit of regularization. And, and now we get back to our, our problem right so which is detection as we said. Okay, so, indeed here we have two tasks. So the initial part of the network is exactly the same we have been discussing till now for classification right we have a party which is the same as the previous then we get to a certain factor I mean towards the end. And then here, we create two heads. So one is dedicated to classification, and the other will be dedicated to prediction of the coordinates for the boxes.
The initial part of the network is the extraction of the feature that they are that you have just one feature extraction extraction part. And that feature extraction so the model is extracting has to be good for both classification and localization. And now, this part here. Well, you can use transfer learning. Right, so you inherit knowledge from a network that has been pre trained on image net. And then you forget about the last part of a network we remove it, keep the initial part, create the two heads train, I mean fine tune at this point because what you are training is really the ending part, but this part is fine tuned. So it's just slightly updated. And you are indeed updating the parameters of this part according to the two objective, which is now dedicated to detection. Now, actually, in the example I was providing there was just one cut. And I mean, this is not really the texture at the end of the localization but it's very, very easy. The problem comes when we have multiple objects. So as far as you predict four numbers and one label then you have four numbers. But then,    mbers, which are continuous values. So, we measure how distance I mean the difference between what we predicted, and the correct box, just with an L2 loss. And here we have for the bounding box part we have a structured output because rather than predicting just one value we have to predict for values. And we are summing these losses.
There's no way in which you can evaluate them all. You have 58 millions boxes in an image of dimension 800 by 600. There's always the possibility that in the bounding box you're looking at there's no object. And so it's neither I mean, none of the C classes so it should be the C plus one class. So in this case, for instance is background. Yes. And then you move the window and you have dog yes cat no background no, then you go on like this. So, yeah, practically you can also calculate the total possible boxes so if you have dimension w of the width of the w for the whole image and hide h I mean uppercase h for the entire image and the lower case as used to identify the bounded box. So we get back to the 224 by 224 because this is the typical kind of resolution we are able to manage with our network. We rely on selective search that provides us with some reasonable number of region. So maybe 2000 as we said, we crop the image I mean the part of the image that we are interested in. We work them. Then we provide the input to a component. And just the comment then produces the label for the content of what has the box or box or the label. Since we are not sure that the selective search is perfectly correct, we propose the convolutional network also to propose a variation of the convalal network. So it produces numbers that tells us how it tells us to transform the original original.
The network is predicting for you the class, but in this extra information it provides you also these variation on the bounding box. So, indeed, is not the one to one correspondence so we have this working in the middle. So the only thing that we can do is reason in a relative way, right, and not directly in terms of the dimensionality of the original boundingbox. So this is due to the fact that we are passing through this warping. And as I was mentioning before, there are many ways in which you can combine the logical, I mean, the evaluating the position and evaluating the classification. Maybe you can have, because what was the issue here is that you have 2000 region proposals. But then for many of them can be very close to each other, but over the same object is not 10 that is a small variation. So you might decide to rank this probability score and just keep the highest ranking total probing box with the highest probability score. So that's what we did here. And then we have evaluating if the class is predicted correctly and if the movement that we propose is better fitting with respect to the ground truth position. And so you just want just to be also be also just so many many many, you want just one bounding. box box box to just be also a small probability score when you predict the class for each of the bounded box, right. So we are not training all the components for for every single proposal right it's just one.
In practice, we post process the detection with these logic. We look for the object of one class. So we might have multiple classes to be able to in our list of possible predictable, let's say classes, we're going to dog cats, I mean, I don't know birds and so on. Now we focus on each class one at a time. And, and we can cancel out some of the bounding boxes with think are not necessary. So how do we do that well we start from the, I'd say the highest one. And then we consider the overlap of these starting I mean that the top, for instance, the top one bounding box with the highest score with respect to the other so the overlap. And if the overlap is so the intersection of a union is higher than 0.7, then we discard the bounded boxes with the lowest lower score. Then we pass to the next one for that class that class. And we just keep the one with highest score for the highest for the class for which we just discard it. And since it is practically overlapping with the orange one practically with the blue one, we just remove it, remove it. So    we can do the reasoning per category, take the top K proposal. And that there is one for which there is a threshold in the background so you can just discard the background. So then there are small details that I have to adjust in the logic of this process but at high level, the procedure is the one that we are we're looking at here.
Non-maximal suppression is a strategy post processing that is adopted in many applications. It is also true that there might be cases where the number of objects that you have an image are so high that there is an ambiguity between the fact that the overlap is due to your prediction model. So, yeah, I mean, the application of an MS depends on the kind of application you are dealing with. And let's say that nowadays considering the abilities of segmentation models that are not only the tech but really pixel per pixel are able to identify the blasts for every single pixel indeed. And then we need to produce this metric which is the average precision. Okay, so, how does it work? So, these are the think about them as a bounding box and focus on a single class at a time so let’s say dog game right for the dog objects in your image you had. And this where the probability assigned for the class dog to each of these bounding boxes. Now, you know that in your images that three dogs, and these orange rectangles here represent the ground truth dog boxes. Okay. You have a point here, and then you go ahead. You see that, indeed there is a match with one of the other dogs that I mean your ground truth provided you. And the matches with an intersection of a union over the five so fine. And out of three ground truth they are correctly assigned so even the recall has been well covered at this point.
The average precision is a value of the intersection of a union that you use to make the calculation. So to take that logic and reuse it for a graph, you have to take care of the edges. And so remember the cocoa challenge. It was the action challenge that run, and so here are the organizers that set to the win for detection. And the output of the output is the mean, which is the average precision in this right so f    oint, so f so f f so   Oint. Right. So yeah here it's changed that three because before it was to so now you're counting over three and so only two have a match, while the recall remains the same. So, well, instead we have these two in the middle that were not assigned. What we would love is that all the correct bounding boxes are all in the top of the ranking of the sorting, let's say of our prediction, and instead all the, let’s say not assigned one are towards the end of the other sorting. Let's say, in terms of score of the prediction, so yeah, we would like to not have false positive detection run above true positive. Right? So, the best possible, the optimal solution would be that all these three green boxes were one after the other. So the last one seems to have a good match. So although your predictor assigned to that bounding box, a low score. So low probability for having a dog in that box. Still, it seems to be a goodmatch so a good overlap with respect to the ground truth.
Region based CNN, RC and N is what we have been discussing till now so the logic we said, we're like this. But it is very slow because it has to elaborate over the 2000 bounding box that the selective search approach provided to the network. And the solution is to switch the application of the CNN before the region selection. So we obtain the tensor I mean it's one of the internal feature maps of our network. Out of these feature map, then we crop the parts. And then we elaborate over them right so maybe resize them we have a just as small CNN, and then we predict bounding boxing classes. This is faster because with respect to so fast RC and then with respect. to the slow version, because we apply the common and just one right so once I mean these are very small parts of the network so not very costly. Well this part here is the bulk of the method, and it's just applied once. So you see, and I'm sure you're ready if you're reading the paper you see this word backbone. So backbone is the main architecture the part of this is extracting most of the feature out of the image. So the backbone is here, and you have a small network at the end. And if you think about Alex net, well, the bulk again of theNetwork, only the ending part is here right so very small part of thenetwork dedicated to the final prediction of classes and boxes, or resnet.
The CNN is also faster in terms of training of course depends on how the structure is defined. And indeed, if you look at the times, we see that. So fast RC and then forget about this as the middle architecture has been proposed in the years. Now, by the time, you can also notice that the bulk of the application to the region is the most costly. So here, the selective search is the very first stage of the seleccions. So, here, I'm talking about the first stage, the first round of the search. And so it happens here right so you can have this Royal I meant before entering the CNN and then obtain the prediction. Still, it remains that here we have a faster solution with respect to this one, because we have this unique let's say application of the Bulk of the network. So this is the the Royal I mean logic. And again, I will not go exactly into all the details is, but I will be just a piece of code, but you need to know what's the logic that it's happening inside, because again you might find yourself in the need of putting a hand inside it to change it for some reason. So I provide more weight to the value that I get out of these. So these are distances 08 this is 05 so this is in the middle between these two. So way to the two values along the x axis, my half and half, and instead here is 08 and 02.
The selective search algorithm, which runs on CPUs we said it's easy, we can trust it like it true we like it but it's quite expensive. So we need to find a way to get rid of the selective search. What we can do is to move this piece that was a, an alien thing that is coming from outside, and do something that the network does predict and learns. So a piece of the CNN. How can we do that? Well, it is a small network that looks at bounding boxes. And then for each bounding box is telling me, is there an object yes or no. So it's just providing me a binary classification so somehow it's, let's say, lighter with respect to what we do here. And it also does a similar work of the end part here so read justice also the original bounding. boxes. Is it faster to pass from past our CNN to faster, which is in a faster, faster CNN. So let's take a break that there is some attention is going down now. Thank you. Thank. you Thank you thank you. thank. you.thank you.Thank you.    tive search that happens here that produces the regions that then I project into the into the feature map. Right. So, indeed, we said before we had the selective. search right which was based on looking at blob and let's. say corners of the image to understand if there is an object or not.
The architecture is called a two stage object detector. The first stage runs one per image, and it's just a region proposal network. The second stage predicts the label for the product region, and the object class and the re organ and positioning of the bounding box. So, it's quite complicated, but one thing that we can do at least is consider the mean AP, so the overall mean AP. And this is on the cocoa data set from 2017 that was trying to do the, this sort of summary considering the overall set on the y axis and the GPU on the x axis. And the takeaways, what can we get out of this? Well, the two stage methods, with the two steps, like faster RCNN, then they get very good accuracy. But they are slow with respect to the single, I mean the one, one, single, one step method. Thank you. thank you.Thank you. Thank. you. Thanks you. for the time. Okay, so we are restarting. We like it. It is faster indeed with respect. to the RCNN. Okay. We are going to restart. We said fasterRCNN. It's faster indeed. It’s faster indeed, we like it, it’ll be faster indeed and we’re looking forward to using it in the future, and I’m sure we will see a lot more of this kind of work in the coming years. Thanks.
In the years then, at least for a certain period of time what happened is that we were experiencing diminishing returns. So then I mean, later on if you have to scale up to 2019. Well, thanks to the fact that now the GPU became faster. So it was possible to train longer than I mean this point started to get higher I mean over the original plot. And you can also manage the issue with resolution of the images by applying a multi scale backbone like pyramid networks that look at the image at different let's say resolution so they were actually. And then sort of a hierarchical way, taking the original image but also scaling up and down and then elaborating over these different scaling version of the image. And so this, yeah, ensemble is the standard solution that always work. Of course, it works also to have more data. So in time, let’s say that the evolution of the detection model. Yeah, there has been lots of evolution in terms of detection models. And let'ssay that nowadays. There's lead and still some interest in terms. of detection because it tends to be cheaper in. terms of cost with respect to segmentation. So before we detect an object and say well we're saying well we’re saying well s    We detect an dog and detect a dog. So this might be useful when you want to mount your model on a,let's say autonomous car, or an as more device for which you don't have a lot of capacity and elaboration.
The process of recalibrating this core is very important, because if we are moving towards trustworthy AI, and we want to trust our model. So, the action we said, and now some anti segmentation. The action localizes the object right and labels the bounding box, some anti-segmentation you want to make a prediction for every single pixel in the image. And so it means that. you want them for the object and you might have some stuff in the background and then all the pictures are labeled as sky. And I imagine that this does not scale right it's not efficient so we are in the usual problem. So what should we do then. Well we can think about a network that takes the full images input. It passes through a certain set of convolutional layers. We can design a way that we tried and patting in such a way. that we get the feature ending part, I mean, after a certain number of layers, with the same extension with respect to the input. Okay, we want this agent to maintain agent with the end in this way. If we are able to do that so if we create a network, if    ort right the prediction and then we take only the highest score the bounded box with the highest scores. That's enough. So how can we get semantic segmentation? Well, again, we can Think about considering windows inside an image, and then pass it through a deep network then predicts a label, andthen we assign the label to the central pixel of the window.
Alex: We can create this kind of design with stride and padding by choosing properly the windows inside the convolutional layer to maintain the dimensionality of the image especially machine ID by the. But the. problem is that for every point, I should be able to have a decision, an area that's a the original area that was covering the whole image. And it's, I mean this is this is not a good solution. It's super expensive. And so it cannot. It is not the right thing to do again. Alex: Well, we can try to compress the information on the image. So, what can we do inside the feature map inside what the logic of what we do. Alex, you know that it goes from something that takes an image to something that has a lower dimensionality is smaller, and then you take th  th    is smaller. And then you have a lot of special channels but in terms of specialality is.   in this way and we are get to this feature map here, then, regardless of how many channels we have at this point, see we can apply an art max or evaluate then the prediction, obtain a map that produces for every pixel, the prediction is the same as before. So now, I said that it can be regardless here, but then when I get to the last layer before the art max. What am I doing is that I want to have this tensor that is properly designed such that this C is exactly the number of classes that they have to predict.
In practice, six year was here so then this tool and appears here. So, then down sampling, we know how to do that, because it was what was there. Now the issue is, how do we do up sampling? We don't have an idea we didn't see it before. There are also ways to run the up sampling. Some of them are not learned, so just strategies. All of this is not learned right so we said all this strategy we're looking at here is just technique, you're not learning anything. There's no way that we are learning right so the model is not. Yes. So going to learnable up sampling just quick reminder of how the convolution work right. So we might have an input like that. And you have the window that is moving over the input takes the dot product and the output of the product that is positioned in the output. And then depending on how you define the windows padding and stride, you might maintain the same dimensionality of the input. And if you move for instance here is the output then you take just stride one, and then you produce the product produce the assigned to the position. And this means that, depending on, how you set the stride and padding then you might either maintain the original dimensionality or you might not. So this is something you can do. And in this way we have the same the center of the network, we have something which is summarizing the information of the whole image.
Transpose convolution is a procedure that increases the size of the input or decrease the dimensionality of a model. It's not a deconvolution, because this is something that we are inheriting from the signal, I mean, the signal theory and so on. And so on we sold the explanation of the convolution and the operation of that operation, and there's a mathemati    and there's the opposite of that, which is the opposite. So, to understand what happens here in terms of some, let's just see one, one dimensional example. So the content of the window is the learnable weights. What do we do we said, well, we have a filter, we multiply a by the filter, and then this which is a result is printed into the output. And here, you take the sum. And then, since our goal we said is for instance here to pass from two to four, then you trim one row. In this case is just one value of two throw it away. Yeah, you crop one pixel out to to obtain this doubling of the dimensionability that you are aiming for. So you have to pay attention in some cases I mean it might be better to take an average, but you just take the procedure to make you just where they just where the average. And this depends on the final dimensionality that you want. So it's just the, this is what, what your target here is that to increase double theSize of the original input.
The transpose convolution is practically what. we are doing in this inverse of the convolution. So this is how the procedure in the architecture should be done, reduce the dimensionality and then without pulling we'll get back to the original dimension. So object detection on the left, just predicting the boundary boxes, and then semantic segmentation on the other side is what we get. Now you see that object detection localizes the separate objects so if I have two cows, I'm able to identify cow one and cow two here it's a multi cementation know is just cow. So we're separating the two instances. So, then we will see that instant segmentation improves over that but one detail is also to take into consideration is that depending on what you're doing detection or segmentation there are the background that's taking different names. So panoptic is doing the two things at the same time separating the instances and notating also the background. And there are lots of works now that focus on this model since we have a model that is able to locate objects. So at this point here I mean that is a cal formulation per se is not what we're doing here. So yeah, it doesn't mean use different kind of name but usually transposeconvolution is the right one. It's called transpose Convolution because yeah it is what they are doing. Yeah, there is a relation with how bad propagation works in the two direction but I don't go into the details of this.
Unsupervised is just creating new images and not about constrained generation. So we have a model that is able for instance to locate people inside an image and also able to predict the key points of the in of the person. And so we have this kind of results where you're locating the person and locating also their joints or p parts of the face. So this is the overview of the things that we have been looking at today. Now, I know you're tired, but it's better to not stop here because otherwise we don't have time to cover some topics. So, let's dedicate another half an hour maximum to at least something of the next block of slide then I will leave you. Okay. Any doubts question about the semantic segmentation parts? Okay. Pacification segmentation detection and instant segmentation. Okay, I'm not talking about conditioning on anything. I want to have the typical thing that you write with text and think about. The typical task is clustering. So I'm seeing learning the logic is just adding these other structure inside the prediction model. So you need the data where you have both a 2D and 3D information, but still is just add an extra head out of the of the. So that's what we are dealing with. We're dealing with approaches for which we supposed to have a ground truth. Right. So the labeling someone provided us the annotation for the object inside the image or provided us with a groundtruth sentence which is the caption, and then we train a model.
Unsupervised learning in the particular case of deep learning model can be useful. So, labeling the data in general is costly. And you know that you want to classify on a certain amount of objects. So we want to exploit unlabeled data to pre train our model, and then he narrated it for transfer we be a transfer learning for another task. Let me provide you some example. What you can do is that take parts of patches of these images. For instance, you take an anchor point I mean a region of the image. Then you take patches around it. And then you ask your network to predict. So the other eight patches, and you ask the network to tell you where which kind of position has this patch here with respect to these one, so in this case you expected to produce as label left. Okay let's say top, upper top, right and you have to combine this information for all of them. Yeah, so you have a upper, upper and you did that. So that is that you did what    y I asked my model to generate a sent an image with what I asked no, I don't want that. I want to model that, given a bunch of images is able to understand the statistics of the data and produce a new sample from the same statistics. But a new one, a new image that was not there in the train set. So these are overall I mean generative models are by definition unsupervised.
Unsupervised learning is the general family of model, I mean, it means training a model without labels without annotating the human annotation uncrafted annotation. And nowadays self supervision is a part of the foundational models because many of those are trained with pre trained with unsupervised data so not labeled through a self supervised task. And one of the most popular self supervised strategy is called contrastive learning. Well, what you want is that so you exploit. multiple crops extracted out of an image. There are multiple crops and these crops also augmented. So there are just two different crops augmented in different ways. And then what you do is that you provide them to a network that produces a feature vector, and then you have an ending part. So I had, which is just a multi layer perception so fully connected layer, and you asked that crops that comes from the same image should be close together so they have to attract each other. And so this is one of these contrastive learnings applied in a lot of foundation model database that's a basis before then doing the transfer learning for specific downstream applications. And it's a more detailed kind of repulsion. Okay, because now it's different, different instances, but I still can do it and the kind of. representation that I had there is super good. So then use it for near neighbor, you see that to close instances are really close even if you look at the images the original images right so it's very very good representation to get out of this.
There are different kinds of families of methods overall, if you want to think about a sort of taxonomy. You have explicit density model implicit density model. Some of the methods that are tractable and still you can really write down the probability distribution. Some others for sale, you cannot really write it down but you can approximate the density. So we'll look at three example, tractable density approximate density and direct so implicit density direct approach, auto regressive strategy in this first family variational and quarter here, and the guns so the generating of the cell network here. So what is the problem, the problem we said is that we have some x, which is our data so someone provided us a set of data images, maybe, and this is the data from which we want to predict the distribution so want to distill out of the data is the p of x, and we expect to do it through deep network that is parameterized by set of parameter w right. So, the probability of these images is an axis and is an input let's say what I want to learn is the probabilities of this data. So they model the distribution they try to models the distribution out of which every single sample has been drawn. You expect that they have a very high likelihood of belonging to that distribution. And then you can think about it about it in the other way around so you have the full probability so you  have the likelihood in the data that you are looking at. So the images that were produced were really really bad, but they were the very first steps towards trying to go towards this logic.
The first family of generative methods were called pixel RNN, and we're following this logic of applying an RNN and interpreting an image as a sequence. During training you can get one pixel at a time and training the network to generate I mean this this pixels, and then at test time, as in predicting a sequence right, you have the start wildcard word. The output of this convolution with the with the input will be the value on in the specific position. So this is the value that you are predicting are just for you can just think about this being 255 so 255 is the great value. So by looking at all this region, left to right you can predict the upper part of this pixel here. And the convolutional kernel that you're using in practice is a combination of vertical window and horizontal window tha    already were able to obtain it. Then, of course, the training sets. Well, they are highly likely, while any other samples that you didn't see during training might have a low likelihood, especially this far away with respect to the training set. So, the probability we are trying to obtain to make our life easy we of course exploit the chain rule. So it's like, out of aggressively producing every single pixel so every pixel depend on the previous one. Still, at the end of the day, what you want is to have this p of x which is parametrized on a set of parameters and indeed we're using an arrow net to do that.
Pixels CNN is likely faster than the pixel RNN, but still it's out of aggressive so it works in subsequent stages. An auto encoder is an unsupervised approach whose goal is just that of recreating the input that someone provided it as so produce as output what I mean the the order providers. After training, you can just completely discard the auto decoder, you throw it away. So, then you can your you can use it to for classification just the training the last part of the network fine tuning maybe a bit this part. You are applying transfer learning for a specific downstream task that you have, and for which you have a limited amount of samples. And it's a probability it introduce some probability in the logic of how Z should be structured, and then will allow us to discard the encoder, select the new Z and select new image that was not there at time. Since now introducing this spin inside the architecture, I don't think it's something we can do at 647, but now at least we know what I saw here at least at least I think we can talk about it next time. What we want to do still is to use this logic for generating new images till now, this structure does not generate new images right is just reproduces the images that so during training, but we want. to use it for generating. new images. So now, variation out encoder are produce a. a. new image, since now introducing a spin inside this architecture requires a month.
The total operation is completed in: 2 minutes and 34.09 seconds