Theory of deep learning is a theory of how to build a model that can make a prediction. We will try to go through the concept and the basis of this theory. We have an input, X, we have an output. And in this case was Y is a label set or we want to classify it. And it's practically the same. We might have a classification task. Will it be perfect? Well, probably not. But we will see that for deep learning, we will have a specific procedure based on that propagation and change rule. And so some guarantees may mean, let's suppose that we adopt this procedure. How good will be the prediction that I'm able to provide? And this is a very difficult question to answer, because anyway, we are starting from data, we're making some prediction. Then how good is that prediction? It's not that easy in the sense that there's all a theory behind that. And here I'm just starting from a very simple example. I'm taking the avocado as an example. You can eat it and prepare guacamole with that only in a very specific period of its life, because otherwise it's either too early or too late. So what you want to do is to collect data and then try to make a model. But in general, the procedure is always the same, the training data, build a models and then make prediction on a new avocado that we are going to buy in the future.
The test error, is it always possible to get 100 percent accuracy? I can already tell you that the answer is no. There's no way to write down any guarantee on the fact that your model will be 100 percent accurate. So we need to define something which is called the true risk, which is written like that that you don't know but you can write it down in a mathematical form. So yeah, you can just bring it like that. So the expected loss considering the value of the X is distributed to according to D. So you know what does it mean to calculate the expected value of X? So that's what we are trying to do. We want to optimize our model on the data that we see during training. But we want to what we care about is our ability to predict in the future. And this is just a 0, 1 loss. So it's a function that measure how many errors you are making. So with a 1. So this loss function is really evaluating the difference between what you have in your mind. So your model that you're trying to create and the ground truth. So that that H is different from f, and the probability of X is at least f of X. So, yeah, in practice, we are minimizing a training error. So what we can do is to define one hypothesis. So our mental model, so the way in which we analyze the data and from the data, we define this H of X, and then we want. to evaluate how many times this H has been different.
In practice it's saying that we are minimizing the number of errors, nothing more than that. The true risk is also called generalization loss. So the problem again is that H is just our model. But we are making an hypothesis. We're trying to understand how this H is made on the basis of the data. So we want to find this H and we want it to minimize this expression. And here we have what we are trying to do is exactly that at the end. So H of X, we are just trying to find the Y that better maximize this. Now, ideally, this is what we know that all this reasoning are based on the bias rule. I mean, and this matches the logic of the bias pacifier. So apparently everything matches with what we have been learning till now. But the problem is that again, it is just theory. We can write it down like that. We have the probability expression. We know that everything is correct mathematically, but we don't really have access to what is defined as bias classifier. And in general, we're never able to provide a better prediction with respect to what the bias classifiers will do. So whatever is the machine learning algorithm anyway, we cannot improve over the bias risk. So if people want to really fit an artificial machine learning model, we should be able to create such a model that a human is also able to take similar decisions. So let's say that if a human's w w w is what a human would do.
Anjum Gupta: We don't have neither the theoretical ground truth F, which is the perfect function, nor the bias classifier, I mean, bias modeling star, we just have H. So intuitively, what we would like to do is to optimize this H for future thing, but in practice, we optimize just on the training data themselves. Gupta: For every learning model, there's always, so even if you are creating a model that apparently works well, there will be always a condition in which that model will not work. So our model can only be probably correct, precisely can onlybe probably and approximately correct, he says. So that's the statement from which we start. And how can we prove that? Well, the problem is that the world again is nuanced, this variable, there’s a lot of uncertainty. So in any case, you have to recalculate the risk study, reevaluate and remodel the empirical risk every time you need to solve a new problem, Gupta says. But anyway, so how does an accurate classifier look like at this point? well, as we said, the best that we canDo is to measure the empiricalrisk. So we have some training data, which are drawn from a certain distribution that we are not aware of, so we cannot access that. And we have our, let's say, M training samples. And what we try to do to minimize the error of the prediction that our model would do on the on training set.
The probability of not seeing data of type X2 is one minus epsilon raised to the power of M, right? So not seeing X2 means just observing X1. The input to the learner is randomly generated in the sense that, again, when you do the experiment, you are experiencing a random variable. So there's always this very small chance or possibility to see again and again, the same type of data, as we said. And although we said we can get to this, there's no algorithm that can guarantee that this will happen for sure. Okay. So the relaxation is that our algorithm might fail with the probability that we call Delta. And also Delta is user-specified. So we have two, let's call it threshold. The one is which tells us how small the bound and the risk is. And the other is related to the probability we can behave in this way. So these are two parts of this logic. So overall, I mean, the thing is this one thing is that the pr  pr is this. So, okay, I cannot say that my model will be perfect, but at least I can say that its risk will be below a certain threshold. And this is a bound. It's a bound on the risk of our model. And we will see also how it is specified. Okay, so we have a model H and the pr is   pr. So this is just an approximation, but you can trust me.
The PAC, which is the PAC theory and PAC theorem, is at the basis of any learning model. It means that we suppose that when we create a model, we know which kind of shape our model should have. So we don't know exactly the model with all these parameters. We just know the shape. On M, epsilon and Delta, there is a relation between M and Delta. And then we can expect that with Delta, we have a theoretical risk at least one minus Epsilon. So this is the probably and approximately correct learning statement. And that's exactly the best that we can do when we try to learn something from our data. And indeed, the key word here is data, because we are supposing to observe M samples. And it's just the empirical risk and we are trying to minimize that. And we have M samples and we're minimizing that. So, yeah, we cannot really get to the risk. The theoretical risk we don’t have access to. And this is again, I mean, the pack theory, so the pack theorem. Okay. So again, the concept here is just complexity.    And it here is called complexity. It is just the complexity of this M, H is just what we have seen before. And of course, this depends on the fact that you open up the data, you look at the data in the eyes and blah, blah, everything we have been discussing in the past.
The cardinality of a model is the number of functions you have inside the family H that contains both the bias classifier and your classifier. The family of lines is infinite, which means that you need an infinite amount of samples, which yeah, it does not sound nice. So there is indeed another step. We are not really dealing with infinite hypothesis classes. So in general, anyway, we are trying to define, let's say, the. number of parameters that overall describe the family of. lines, I don't know, something like that. It's just a matter of how many parameters we use for each model. So here we're just a few examples. I will not ask you to remember all of them, but if it happens that I ask you what is the PAC learning theorem, I mean, what is PAC stating, why are we talking about that? I'm expecting you to know what's behind this. Okay, question, doubts, everything clear? Less? Okay, so now I would like to quickly go over something that many of you already know, because we will discuss a bit about cross-validation. This can be useful for someone else who did not have the occasion to see the concept before, but it will be quite practice, yes, have a t    's the important, let’s say, target that I’m trying to provide you. So to have guarantee on the ability of a models to make prediction, we should have enough data.
Underfitting means that possibly you are using a limited amount of parameters here. Overfitting means you could use more, but then if you use too much, then you start diverging. So these are typical example of cases in which you are overfitting. The capacity, which is on the x, is the number of parameters of your model, and the error is on. the y-axis. So what you expect is that during training, we said, we are minimizing the empirical risk, so we are optimizing our model on the training set, so the error goes down and it goes on being smaller and smaller. But the problem is that instead, in terms of ability of predictor for the future, the validation error, the generalization error, then after a while, rather than reducing it, so rather than fitting well our data, we start overfitting, and so the model will not be able to predict correctly in the future. And you know, there are several ways in that there are which you can avoid the overfitting condition, one of these cases is by applying cfitting to your data set. So yeah, if you don't have enough data, of course, soThe issue is related to the number. of parameters and the. number of data as we discussed here now. But now you know that this PAC theory exists, we have a definition of how many samples we need to access, but even if you know the theory, things can go wrong that way.
Cross-validation is a way of managing your data set. You can separate training sets and validation set like that. There are other cases, other things that you can do. For instance, you can apply what is called the cross, K-fold cross- validation. This is where you train on K minus one parts and you leave one part outside. And that's the result of this process is the optimization of the parameters. So question, any idea of when the leave one out cross-validated is essential? So that'sThe best thing to do. You know what does it mean to have an estimator which is unbiased. So it's a good estimator. And when I say we get an almost unbiased estimator of the generalization error, it means that the evaluation that we get by that process, provide us an exact, so a very good indication of how our model wil be used. So we are doing exactly the same thing that we're discussing about probability, we were doing with maximum likelihood or maximum posteriority. No idea why. Any hypothesis? Yeah, when we have a very limited amount of samples, it's inevitable to consider just one sample. So leave just one of the sample outside. Okay, so you do it. Because this is, yeah, here I'm indicating the number of samples. So in practice you are searching for the parameters that better fit the more of the data themselves. And so you repeat in practice the evaluation K times, and then you take the average.
The pack theory is a theory that says you should never access the test set while doing your optimization. The realization, which is called the realization, starts from the concept of the Occam razor. So anyway, we are close to Halloween, so it's a good message. Okay. Now, so what we are, so where am I going? What I want to say is that you can train a model optimizing the parameters on your data and trying to get the minimal training possible. So again, there's no guarantee at the moment on the ability of the model to predict in the future. And so this concept of being able to re-evaluate the leave one out error, even when we are dealing with millions of data inside the deep network that has trillions of parameters is extremely important. Yeah. So yeah, so we are trying to optimize our H as we said, over the training sets by minimizing when we get a risk. So now, besides the, besides doing a proper cross validation or adjusting the parameters of themodel in the proper way, there are other strategies to avoid overfitting. Okay, so this just the name. SoAnyway, the never, when I say never use the testSet is this kind of never here. So Anyway, we    l perform at test time. If you access the TestSet in practice, you are cheating. If we optimize the parameter through left one out cross-validation, we're almost sure because it's almost unbiased estimator that our model will generalize well.
In the loss, I mean the objective function that we're trying to optimize, we said that everything we are trying to do is to minimize this thing here. So minimize the number of errors that we have on the training set. But it doesn't make sense to use the zero one loss, as in the case that we were looking at before. So what we can do is add something else, which says, yes, I want to minimize the error, but at the same time, I Want to Minimize the complexity of my model. And how can we do that? Well, we can just introduce extra condition in the loss. So we go through linear regression quickly just to connect with this concept of minimizing the complexity and introducing the extra-terameter validation. And yeah, sorry if there is a lot of changing of notation of M, M, but you understand this is theNumber of samples that we are observing. And we are just taking the average or the sum of all the distances between what I predicted and the ground truth anyway. Right? So the errors now is measured in the difference of what I predict and what I actually get. And so it's continuous. So the loss that we prefer to use, and it makes sense to using in case of regression, is the difference between what you predicted and what you are actually getting. Okay. So this is what we need to do. Okay? So one is? Perfect. So now we have this and in this way we are performing an empirical risk minimization task.
So we have a model which is defined by W and B. And which Y is this model predicting? Well, you give X as input, you take the dot product between W and X, you sum B. So this is the Y hat that you are predicting. Then you have Y hat minus Y taken squared, which is what we are trying to minimize. Okay. So now this thing here can be written also in matricial form, just to play a bit with the math. So its behavior will be a parabola anyway. So we can easily search. Well, in practice, the problem is not in terms of X, it's in. terms of W. So you're searching for the parameters W. Now this W contains both W andB. So it's aParabola in terms. of the coefficients. And you can always derive this expression to search for the minimal value because. you can expect this shape to happen. So yeah, so this is nothing more than the prediction. Now, I call this class depends on these parameters is an infinite. I mean, this is an Infinite family, but we know that according to the discretization tree, we can use that. Right. Now you have to take the derivative of the X and X is taken and transposed due to the matr    ng the logic of least squares because then these distances taken squared just to have a positive value. So raise the power of two.
The concept of regularization is related to the concept of performing the cross-validation and the analysis. So in general, let's recap. We said that we need to be able to make good prediction for the future and to at least try to make sure that this happens. We need to take care of how many samples we have and how we tune our models based on the data that we have access to. So first of all of all, pay, how we manage the attention to the data is how we perform the t- validation and analysis and that's extremely important and it helps  truly understand the results of our research. So we can write it down in matricial form. We can take derivative and put the derivative equal to zero. And we can get practically to the, at this point, we just have to derivate and put it equal tozero. This is called ridge regression. Since we have also this regularization term. So it's not just some that linear regression. It has this regularized term out there. So yeah, it's just now you are really looking at it in the eyes considering one sample element at a time. And then you have the same expression because it's raised to the power of two, the two becomes a coefficient and you will have the exponent minus one. Okay. When is this zero? Well, you have to put thisequal to zero, do some steps and you get, this is just a linear system of equation, but you can verify that the solution is this one.
The concept of regularization has multiple kind of, it can be applied in different ways. We can regularize the model, for instance, by introducing something which is called weight decay. Another important thing that you can do when dealing with deep model will be also to add the data. So you synthesize data actually to increase the amount of samples that you see. Introducing randomness is a sort of, is an approach to regularize your model. So we'll see more of them, but this concept of introducing something extra with respect to just the empirical risk minimization that tries to pick the training samples is extremely important to avoid overfitting. Okay, so in practice, here is just a summary of what we've been seeing today. And I think that, by the way, it's also my last slide. And anyway, I would stop here, although we are still 20 minutes, but I mean, I don't think it's needed that we overfill the lecture with lots of stuff. So better that you digest each concept one at a time. So if I ask you what does it mean regularization, please don't get confused in a sense that regularization is a very high level concept. What I mean is that I want to know the whole story. And okay, if you want to provide examples in terms of regression, I will be super happy at the example. So this is just the starting point. We will see more more of that later on.
The bias term, it's not the one that we've been. The bias term you mean is W0? Yeah. Here, yes, in the matrix form when I discuss this, it is. But then when I restart with the calculation here, it isn't. Here is not there anymore. And then I re-add it later on in the later slide. So yeah, I would really stop here for today and we will go ahead then. Okay. Yeah. Yeah, I want to say that now that I apply for UBSA, the programs and for the fall of 2025. So I talked to some professors for my PhD and they asked me if I have a supervisor or not. I know. I can't. I don't do it for any series. So, but this does not mean anything in the sense that you can, you're perfectly free to search for a company that already today guarantees a position for you as a master phase student. You can do that. So the only difference is that I'm not accepting students in my lab as students for master phases before the exam. But if there is another company or another lab that can host you and eventually you need me as internal supervisor, then I can. Pay attention that case. It's just that I want. to write a paper and write the proposal by myself and you have you. as a supervisor. But, you know, I can start the web me that I started. Can you please have a lease of already exam which you passed, right? Yes.
The total operation is completed in: 93.8479 seconds