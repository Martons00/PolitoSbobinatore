 Okay. Okay. Sorry, apparently there's only one projector is working and why, because everything is set to the two projectors should be visible anyway. So even from home, you should be able to see the screen and hear me. If you can please confirm that. Great. Perfect. So we can start. So in the lecture of today, there will be still some mass. Okay, and all it's not. Everybody likes math, but it's just to have, again, concepts, right, so we will try to extract the g's out of these concepts. Okay, so. We saw the night by a stutter, right, so we know how to define our very first classifier based on probabilities. So we have a function of this of Excel, and this is the function we are interested in. And in the particular case of native bias classifier is just the art marks of the conditional probability of pure of X and Y multiplied by the prior probability of pure white. And we said that for the bias condition, we are taking all the elements of the samples that we saw of the components of a sample actually as independent features. And this is what we get in terms of our marks. These different post parts. With the mouse here. Okay, but now the focus is on this. So this is just a function of the input and we are calculating it by maximizing another expression. This is now on probabilities, but if we make a jump, pass forward jump towards deep learning, what we expect is to have practically an analogous condition. So we have an input, which is our X. So we have an output, which is our Y. And we have a function that connects X to Y. And this function is defined by some parameters. So as in the case for probabilities, we have some parameters that we can try to estimate through maximum likelihood or through maximum posteriori. I mean, there are several procedures. We will see that for deep learning, we will have a specific procedure based on that propagation and change rule. We will get into the details later on, but in practice, logic is the same. We have an input, X, we have an output. And in this case was Y is a label set or we want to classify it. And it's practically the same. We might have a classification task. Now. Okay. So the problem is that we would like, I mean, this is a kind of approach of how we want to define a learning model. Now, we would like to have some guarantee on the ability of these models to make a prediction. And so some guarantees may mean, let's suppose that we adopt this procedure. How good will be the prediction that I'm able to provide? And this is a very difficult question to answer, because anyway, we have a model, we are starting from data, we're making some prediction. Then how good is that prediction? It's not that easy in the sense that there's all a theory behind that. We will try to go through the concept and the basis of this theory. And here I'm just starting from a very simple example in the sense that I'm taking the avocado as an example, just because it is a very complicated fruit. You can eat it and prepare guacamole with that only in a very specific period of its life, because otherwise it's either too early or too late. So it's not that easy to identify when the right period, so if the avocado is ripe or not. And so what you want to do is to collect data and then try to make a model that takes a prediction. But yeah, how good will be this prediction? What you do is just you collect data, you try to analyze the features of these samples. And I don't know, it can be the color, it can be the softness. You create your own mental model of how to understand if the avocado is ready or not. And then you try to make prediction. Will it be perfect? Well, probably not. But in general, the procedure is always the same. Training data, build a model and then make prediction. OK, so the interesting aspect here is that what we would like to do is to make a prediction or be able to make a prediction on a new avocado that we will buy in the future. But we are basing our reasoning and our model of something that we have seen in the past. So what we are trying to do when creating our model, mental model, is to minimize the training error. So we want to, I mean, every time you buy one, you go home, you open it up, you prepare it for lunch, for your salad, and then you realize if it's good or not. And on the basis of the result of this evaluation, you understand how to annotate, let's say, the feature of that specific avocado that you bought. So, yeah, in practice, we are minimizing a training error. So we are trying to optimize our model on the data that we see during training. But we want to what we care about is our ability to predict in the future. So the test error, is it always possible to get 100 percent accuracy? I can already tell you that the answer is no. I mean, there's no way to write down any guarantee on the fact that your model will be 100 percent accurate. And so the question is, what should we focus on? What are the important parameters which are really characterizing the learning problem? So since I know that this is the situation, I'm training a model, I'm defining a model on something which I've seen in the past, but I want to predict in the future. So what are the important things I have to care about when I try to define this model to make sure that at least I will commit a limited amount of errors in the future? So we try to formalize this a bit. OK, so X is our input space, Y is our output space. We want to we are focusing on a classification problem and we have K classes. So in this particular case is ready or not. So it can be binary. So when it is binary, the problem you have Y, which is 0, 1 or minus 1 plus 1. And in case it's multi-class, you have Y, which goes from 1 to K. And yes, you have this set of data and you try to create your model. So X represents the measurement of the object. So a vector with the features and Y is the label. So the category the class belongs to. Now, your data are drawn from a distribution D that you actually don't know. So there is a distribution D from which the data have been drawn, but actually you don't have access to that. You have access only to the samples that you are observing. So to your observation, you also suppose that there is a function that connect X, Y and that will allow you to make prediction. But again, you don't know this ground truth function. So the correct one, you assume there is one, but you don't have access to that. So what we can do is to define one hypothesis. So our mental model, so the way in which we analyze the data and from the data, we define this H of X. And then we want to evaluate how many times this H of X is different with respect to the F of X. Now, F of X is also Y, right? So whenever I have an X, this will provide me with a ground truth. So this is the correct label. And this is just a 0, 1 loss. So it's a function that measure how many errors you are making. So this loss would be 1 if H of X is different from Y and it would be 0 if H of X is different from Y. So you count any time you make that mistake, right? So with a 1. So this loss function is really evaluating the difference between what you have in your mind. So your model that you are trying to create and the ground truth. So this is the correct label. So this is the correct label. So this is the correct label. So this is the correct label. So this is the correct label. This is the correct label. And so I'm evaluating the difference between what you have in your mind, so your model that you are trying to create and the ground truths. So we need to define something which is called the true risk, which is written like that. that you don't know, but you can at least write it down in a mathematical form. So these your distribution are the risk and is the probability that H of X is different from f of X. So yeah, you can just bring it like that. So the true risk can also be written like that. So the expected value of the loss considering X distributed according to D. So now you know what does it mean to calculate the expected value. So now you understand what this expression means. Right. We don't have the distribution D. So you cannot write in the full form. I mean, you can write in full form, but we're not able to calculate that because we don't have the P of X, Y. Right. We don't have the joint probability. We don't know how it is, its form. But when I say expected value, I, we know that it's just a summation or the integral. So now we are supposing that this is a discrete distribution over X and Y distributed according to D of this point, the loss function multiplied by the distribution. The true risk is also called generalization loss. So there's a reason why you have this general loss here. So the problem again is that H is just our model. But we are making an hypothesis. We're trying to understand how this H is made on the basis of the data. So we want to find this H and we want it to minimize this expression here because in practice it's saying that we are minimizing the number of errors, nothing more than that. So the probability that our prediction is different from the correct one. So this is what we want to do. So, yeah, minimizing the number of errors means that then, which is the other way around. So it's also maximizing the number of times we consider the prediction correct. Right. So these two are our equivalent. And here we have what we are trying to do is exactly that at the end. So H of X, we are just trying to find the Y that better maximize this. Now, ideally, this is what we know that all this reasoning are based on the bias rule. I mean, and this matches the logic of the bias pacifier. So apparently everything matches with what we have been learning till now. Yeah, there's still the problem that we don't have H. So we can consider H star as the real bias classifier. So the problem is that even the bias classifier per se is a sort of ghost that we need to fetch maybe. But the problem is that again, it is just theory. We can write it down like that. We have the probability expression. We know that everything is correct mathematically, but we don't really have access to what is defined as bias classifier. And in general, the bias classifier, it would be the perfect one, or at least the theoretical best that we can do started from the data. But in any case, we don't have access to this H of X. Our risk will be, so the risk of our real H, the one that we are doing with our hand, we are trying to define, will be, this risk will be anyway higher than what the bias classifier will do. So this is called the bias risk. And in general, we are never able to provide a better prediction with respect to what the bias classifier will do. So whatever is the machine learning algorithm anyway, we cannot improve over the bias risk. Now, again, are we fetching a ghost? So what is this bias classifier? What does it mean? Well, it turns out, so we don't know that the bias classifier, the end of the story, but we try to approximate it, or again, create a mental model of the world to be able to reason on the basis of this math. And from research paper and discussion, what comes out, not only from, let's say, computer science, but in general, so from developmental scientists who study humans, it turns out that as a human, we are inherently wired to reason in bias terms. So our mind, when we were saying before, we buy avocado, we create a model and then we understand how to buy it, the next step is to buy a model. And then we understand we buy avocado, we create a model and then we understand how to buy the next avocado based on the features that we have learned before. Our mind works on the basis of this bias theorem, so on the logic. So practically, it has been shown by the way in which people behave that we really fit into that model. So the best that we can do is to suppose that if we want to create a machine learning model, so an artificial model, we should, we would be able to create such a model if a human is also able to take similar decisions. So let's say that what a human would create in terms of annotation, in terms of prediction, is exactly what we try to replicate and to mimic with a machine learning model. Okay, so this is, you see that there was this tip, Anjum is always, I mean, the person that I'm mentioning several times is the person that is behind Coursera and so one of the leaders in AI since a lot of years. So in one of his papers, just say that the bias risk can be just estimated with what we expect from human performance. So if a human is not able to perform a task, for sure, a machine model will not be able to do that, and on the other way around. Okay, but this said, anyway, the problem is that we don't have neither the theoretical ground truth F, which is the perfect function, nor the bias classifier, I mean, bias modeling star, we just have H. And we are creating this H on the base of the data. So intuitively, what we would like to do is to optimize this H for future thing, but in practice, we optimize just on the training data themselves. So luckily, we have this hypothesis that we discussed in previous lecture about the IID condition. So at least we can state that if our data is behaving according to the IID condition, so identically and independently distributed, so all the data are actually are drawn from the same distribution. So what we expect is that whatever model we try to create, so when we learn an algorithm, then at test time, it will be, let's say, the data that it will see at this time will be of the same distribution of the data that has been seen during training. And this is extremely important. Without this assumption, we cannot make any, we can't say anything about our ability to predict in the future. But anyway, so how does an accurate classifier look like at this point? Well, as we said, the best that we can do is to measure the empirical risk. Okay, so we have some training data. So we don't have access to the true risk. We only have access to the empirical risk. So we have some training data, which is this one. And we know that they are drawn from a certain distribution that we are not aware of, so we cannot access that. So we have our, let's say, M training samples. And what we try to do is to minimize the error of the prediction that our model would do on the on the training set. So this is the empirical risk. It's not the true risk. We cannot access the true risk. But the best that we can do is to make and calculate something from the data that we have. So this also means that, I mean, there's no one single model that fit all the possible tasks. So in any case, you have to recalculate the risk study, reevaluate and remodel the empirical risk every time you have to solve a new problem. So, yeah, for every learning model, there's always, so even if you are creating a model that apparently works well, there will be always a condition in which that model will not work. So this you have to be aware of, of course, there's no free lunch in this sense. So our model can only be probably correct, precisely can only be probably and approximately correct. So we try to define exactly what does it mean. So even if the risk produced by the bias classifier is zero, we cannot hope to find an H such that its risk is zero. So that's the statement from which we start. And how can we prove that? Well, the problem is that the world again is nuanced, this variable, there's a lot of uncertainty. So you might be in a condition in which, for instance, your data are of two types, type X1 and type X2. And you have the probability to experience and to observe data X1, which is equal to one minus epsilon, and the probability to experience X2 with the probability with nine, which is equal to epsilon. So you, I mean, let's put ourselves in this kind of condition. Two types of data, one of the one type of data has probability to appear in my observation equal to epsilon. And the other one is, the probability is one minus epsilon. Now, if I take, so what I do is that I make M observations, right? So M experiments and I obtain M samples. IID, they are extracted from this distribution that actually I don't know, but I mean, it's just the shape of the distribution. So the probability of not seeing data of type X2 is one minus epsilon raised to the power of M, right? So because the probability for X2 is epsilon, so not seeing X2 means just observing X1. X1 has probability one minus epsilon. If I take M samples and they are always of type X1, then it means one minus epsilon raised to the power of M. It's like having a bag with blue and red balls. And whenever I pick, I mean, I pick M time balls and they always come red for some reason. They were only one of the two types. So one minus epsilon raised to the power of M can be approximated through Taylor expansion. This is not relevant, but you can trust me. You can write it down as the exponent of, so E raised to the power of minus epsilon M. So this is just an approximation. So in practice, if epsilon is much smaller than one over M, then in all our experiments and in all our observation, we'll never get to know the shape of X2, so how X2 appears. So again, suppose you have a bag, you're just picking balls out of it. You do an experiment of picking M times, you always see red balls. You can make the hypothesis that the bag is full of red balls. You never saw a blue ball. You don't have any experience of that. So although this might be rare, but it depends on the ratio between how many balls you have in the bag. So this might be a rare case, but it's still something that might happen. And if this happened anyway, you are in a condition which you cannot, I mean, you don't have observation of the full distribution, observing just one part of that. And anyway, your prediction for the future will be wrong, because if you have seen M time red balls, then you expect, I mean, you create inside yourself the expectation that the next time you pick another ball, it will be red. Okay. So what we can say is that although we don't have this, we cannot state that the risk, I mean, it's impossible to create a model for which H is equal to zero, even though, I mean, the bias formula, I mean, the bias logic, the bias that's a perfect one would be risk zero, even if H is of the same family of the bias, and I will go over that, of what does it mean. But even in that case, anyway, we cannot get to this. The maximum that we can do is to get to this. So we'd be happy with being able to state that the risk of our model is lower than a certain threshold. So lower of something. I mean, this is a bound. It's a bound on the risk of our model. And this already starts providing us some guarantee, right? So, okay, I cannot say that my model will be perfect, but at least I can say that its risk will be below a certain threshold. And this threshold is user-specified, and we will see also how it is specified. Okay. Now we said we can only be probably correct, but also we can only be approximately correct. So the input to the learner is randomly generated in the sense that, again, when you do the experiment, you are experiencing a random variable. We have been discussing about what are the random variables in the last phase with the distribution and so on. So there's always this very small chance or possibility to see again and again, the same type of data, as we said. And although we said we can get to this, there's no algorithm that can guarantee that this will happen for sure. Okay. So the relaxation is that our algorithm might fail with the probability that we call Delta. And also Delta is user-specified. So we have two, let's call it threshold. The one is epsilon, which tells us how small the bound and the risk. And the other is Delta that is related to with which probability we can state that our risk will behave in this way. So these are two parts of this logic. And the probability, of course, will be over the random choice of examples. Okay. So overall, I mean, of all these lines, the important thing is this one. So we have a model H and the probability that the risk of this H is below epsilon. So this probability will be higher than one minus Delta. So this is the probably and approximately correct learning statement. So this PAC, which is the PAC theory and PAC theorem, is at the basis of any learning model. So this is the kind of guarantee that we can have for every learning model. And that's exactly the best that we can do when we try to learn something from our data. And indeed, the key word here is data, because we are supposing to observe M samples and epsilon and Delta. So these thresholds, I mean, this concept that we are introducing in the PAC are exactly, I mean, they depend. So our freedom in defining them depends on how many samples we saw. Okay. On M. There is a relation between M, epsilon and Delta. And that's the interesting part, actually. Now, we made a lot of relaxation, let's say, in this PAC theory, as we saw before. And one of these, let's say, assumption, a bit hidden, but I'm putting it now on this slide, is the fact that we are adopting this reliability assumption. What does it mean? It means that the H function we are defining behaves to the, as it belongs to the same family of the bias classifier, the bias model. So when I said H, we don't know it, but we suppose that it belongs to the same family of function of H star, which is the bias module, bias model. So what does it mean? It means that we suppose that when we create a model, we know which kind of shape our model should have. So we don't know exactly the model with all these parameters. We just know the shape. So here are two examples. For instance, I can say, okay, I suppose to know the shape of my model, it can be a line, or it can be the family of lines, or the family of squares. It can be the family of circles. So the kind in terms of shape of how we parameterize our model, so the function, high level function that we use to define our H, we suppose that belongs to the family of lines and the family of squares will be the same, will be the correct one in terms of bias classifier, and will be exactly the same that we would choose for our H. Of course, it's a simplification assumption. So the important here, we're just saying we know what's the family of the bias classifier and we are stating that whatever will be our H model, it will belong exactly to the same family. So if we start saying, okay, for this problem, for sure, the bias classifier has the shape of a line, then okay, this is our hypothesis in practice, but it is reasonable. And of course, this depends on the fact that you open up the data, you look at the data in the eyes and blah, blah, everything we have been discussing in the past. Okay. So, yeah. So with the basis of everything we have been discussing till now in practice, so within the back learning theory and with all the relaxation that we defined, then in practice, we are saying that we are trying to state that whatever we learn by minimizing the empirical risk will anyway be solid enough to provide us a future prediction. So prediction for the future. So this is nothing more than what we have seen before, right? It's just the empirical risk and we are trying to minimize that. And we have M samples and we are minimizing that. So, yeah, we cannot really get to the risk. So the empirical risk might get to zero. The theoretical risk we don't have access to. And this is again, I mean, the pack theory, so the pack theorem. And this is just to highlight the relation between M, Epsilon and Delta. So for every distribution such that the eligibility assumption holds in H, we're running the learning algorithm with a certain amount of data that is higher than this. So you see that Epsilon, Delta practically define themselves as sort of threshold in terms of how many samples I need to observe. And then we can expect that with probability of at least one minus Delta, we have a theoretical risk which is below Epsilon. So again, the concept here is just that this M, H is called sample complexity. And it's the important, let's say, target that I'm trying to provide you. So the important information I'm trying to provide you with all this pack learning theory. So to have guarantee on the ability of a model to make prediction, we should have enough data. It's always the same story. Right. Everything depends on how many data you have. And so to be able to make this statement about the probability of prediction in the future, the theoretical risk and so on, you need to have at least a certain amount of samples. So I don't go into all the theory and demonstration of theorem and so on, but at the end it comes out that you can express this M in this way. So it's the logarithm of the cardinality of the family H divided by Delta, so divided by Delta and everything over Epsilon. So what does it mean, this term here? So this term here, when I say the cardinality of this curly H, and in practice is the in practice is the number of functions you have inside the family H that contains both the bias classifier and your classifier. Because as we said, your H is supposed to be extracted from the same family of H star. So this is the family, it's just the curly H is the whole family containing all the possible lines that identify the classifier. Among all these possible lines, there is the H star, so the bias classifier, and you're expecting to extract from the same family also your H, which is the one that you are able to produce. Now the problem is that the family, this family should have infinite dimension. I mean the family of lines is infinite, you are not controlling that, which means that you need an infinite amount of samples, which yeah, it does not sound nice. So there is indeed another step. We are not really dealing with infinite hypothesis classes. So here we're just a few examples. So they can be linear separated, they can be threshold, they can be region. Although apparently these are infinite, so each of this family can have infinite amount of elements, the truth is that we are letting a computer to define these models. So everything is discretized in any way. We are dealing with bits, right? So if everything is discretized, if we have a number which is expressed by 32 bits, then these are used for each parameter, then in practice we have two to the 32 possibilities to define this parameter. So in general, anyway, we are, and this is trying to define, let's say, the number of parameters that overall describe the family of lines, I don't know, something like that. It's just a matter of how many parameters we use for each model. Let's say that we use just 32 bits to define the number of parameters, then these are the possibility. So the cardinality of the family will function not really infinite. It has, I mean, a finite dimension. So it's not true that we need to collect an infinite amount of samples. We can deal with a limited amount of samples. And so here there's just a practical example calculation. So this was just an instance of what can happen. So, and you get to the famous rule of thumb that you need to have at least 10 times the number of samples with respect to the number of parameters of your model. So you see now why there's a need, it's a long story in terms of theory, but it's the explanation behind why we want a certain amount of samples when our model has a certain amount of parameters inside. Okay, so I hope this was reasonably clear. There are multiple steps. I will not ask you to remember all of them, but if it happens that I ask you what is the PAC learning theorem, I mean, what is PAC stating, why are we talking about that? I'm expecting you to know what's behind this. Okay, question, doubts, everything clear? Less? Okay, so now I would like to quickly go over something that many of you already know, because we will discuss a bit about cross-validation. This can be a recap for someone, it can be useful for someone else who did not have the occasion to see this concept before, but it will be quite quick. So in practice, yes, we have a theory, we perfectly know that this PAC theory exists, we have a definition of how many samples we need to access, but even if you know the theory, you know, things can go wrong that way. So here, we're just considering some set of samples and we are defining as a model, we're using as a model just the polynomial function. So we are taking x, x square and x to the power of m, and we are just, so our model has this shape here. So it's just a polynomial of degree m, and we are using that to describe the data. Now, this is the ground truth, that's what we expect in terms of green line, but depending on the fact that, depending on the number of parameters that I practically use for my model, then I can have different kinds of behavior. So if I take just m equals zero, so in practice, if I just take a coefficient, that will have a line and this, of course, is not a good fit for the model. If I grow m and I take w0 plus w1x, so I get up to m equal one, I might have another line, I mean, not just a coefficient, not just a constant, but a line, but even this line is not nice, so it's not fitting well our data. And then we can, I don't know, use m equals three, and so we have this very nice curve, okay, although it's not perfectly fitting the data, but it's reasonably matching the shape that we expect. Now, of course, if you go to m equals nine, then you have something like that. So too many parameters, so we are, our model is passing through all the data, all the points, but this is not exactly what we want, because although our model is perfectly good in representing the samples we are observing, so at training time, we are having a very, very small error, right, it's zero, we are perfectly good, then at test time for any new sample, we will not be able to predict correctly. So if we have a new point here, I don't know, or here, right, a new x, then our model will predict this y, which is be extremely wrong with respect to the true, let's say, behavior, which is the green line. So these are typical example of cases in which you are overfitting. So the general phenomenon is the thing here you are observing. So the capacity, which is on the x, is the number of parameters of your model, and the error is on the y-axis. So what you expect is that during training, we said, we are minimizing the empirical risk, so we are optimizing our model on the training set, so the error goes down and it goes on being smaller and smaller, even if we increase the amount, I mean, the capacity of our model with more and more parameters, then anyway, I mean, it will be smaller and smaller. But the problem is that instead, in terms of ability of predictor for the future, so the validation error, the generalization error, then after a while, rather than reducing it, so rather than fitting well our data, we start overfitting, and so the model will not be able to predict correctly in the future. So these are the typical condition which you have underfitting on the left and overfitting zone on the right. So underfitting means that possibly you are using a limited amount of parameters here, you could use more, but then if you use too much, then you start diverging. Okay, so yeah, this is just the definition of overfitting when your predictor has an excellent performance on the training set, but the performance on the real world, when you apply the test on, it's quite poor. So yeah, if you don't have enough data, of course, so the issue is related to the number of parameters of your model and the number of data as we discussed here now. But now you know, I mean, on the basis of the discussion that we made, that overfitting is not about the fact that your training error is zero, but it's related to the fact that you are actually minimizing the empirical risk and not the true risk, which is ideally what we would like to have. Okay, so yeah, it's just a concept. And you know that there are several ways in which you can avoid that. So the overfitting condition, one of these cases is by applying cross-validation. So cross-validation, one thing that you can do is to properly manage your data, right? So you take your data set, you divide it in training and test, and then you apply the cross-validation just in the training part and the test, you don't use it. Okay, the test you don't see it, you don't have access to it, you have to keep it out from your training process. Then if you are lucky enough that you have enough, good luck that you have enough training data, then you divide the training data into training and validation and all your model will be training on that. So adjusted on the training, so you model the, define the parameters of your model by the number of parameters of your model by learning on the training and validating on the validation. And so progressively your parameter will be adjusted this way. And once the training is completed, then your model will be tested on the test. So only after this stage. How do you perform cross-validation? Well, we said true, you can separate training sets and validation set like that. Okay, but there are other cases, other things that you can do. For instance, you can apply what is called the cross, K-fold cross-validation. So you take your, let's say, subpart of your training data, you divide it in multiple parts, you train on K minus one parts and you leave one part outside. And so you repeat in practice the evaluation K times, and then you take the average. So in practice you are searching for the parameters that better, here what are we doing? We are exactly searching, starting from the data, the parameters that better fit the more of the data themselves. So we are doing exactly the same thing that we're discussing about probability, we were doing with maximum likelihood or maximum posteriority. I mean that logic here, let that logic there, here we are doing the same. We are trying to choose what are the estimators from my parameter of the model and I'm doing it by fitting the data. So I create a model and then I test it on the validation. So I validate it, adjust the parameter, change them again. And that's the result of this process is the optimization of the parameters. And I don't go into the details of the algorithm because many of you know. Now there is one case, a particular case that is called the leave one out cross-validation. So maybe this is just an easy question because many of you have seen cross-validation before. And so leave one out means that if I have m samples, I leave just one sample out. I train on the m minus one samples and I do it in turn for every sample my training set. So question, any idea of when the leave one out cross-validation is essential? So that's the best thing to do. So I'm sure you know k-fold cross-validation. I've met most of you. But why instead would be meaningful to apply leave one out cross-validation? So rather than dividing the training set in blocks and training on k minus one and test on the part remaining, you do it but your k-fold in practice, the k is equal to exactly the number of samples of the species. Yeah, so you do it. Because this is, yeah, here I'm indicating the number of samples. No idea why. Any hypothesis? Yeah, when we have a small data set. So if you have a very limited amount of samples, it's inevitable to consider just one sample. So leave just one of the sample outside. And that's important. The important aspect is that actually there is another theorem behind this kind of claim is that the leave one out cross-validation is an unbiased or almost unbiased estimator of the generalization error of your model. So now you know what is biased because we discussed. You know what does it mean to have an estimator which is unbiased. So it's a good estimator. And when I say that by running cross-validation, we get an almost unbiased estimator of the generalization error, it means that the evaluation that we get by doing that, by doing this process, provide us an exact, not exactly, so almost a very good indication of how our model will perform at test time. So we are just training our model. And if we adopt leave one out cross-validation, the way in which we are optimizing the parameters, we are providing us to bring us in the direction of a model that is producing, let's say, we expect it to produce the best possible result at test time because we are, although we are minimizing the empirical risk, if we optimize the parameter through leave one out cross-validation, we are almost sure because it's almost unbiased estimator that our model will generalize well. So this is a way in which you can tune your model with the proper strategy. Now, of course, there's always this doubt of should they present this concept yes or no, because anyway, nowadays people say, well, but anyway, we have millions of data, who cares about that? There are recent papers discussing how to use this concept when you kernelize a deep learning model. So all the world of super vector machine dealing with kernel is not that. And so this concept of being able to re-evaluate the leave one out error, even when we are dealing with millions of data inside the deep network that has trillions of parameters is extremely important. So again, there's no guarantee at the moment on the ability of the model to predict in the future. And this can be a good direction to go. So I think it's worthwhile and it's valuable to know this kind of things. Okay. So anyway, we have a training evaluation and a test set. So yeah, so we are trying to optimize our H as we said, over the training sets by minimizing when we get a risk. Yeah. And we have said all this story about the pack theory. Well, here, I mean, the concept is just, we don't want to access the test set. I mean, never access the test set while doing your optimization. If you access the test set, let me go there. If you access the test set in practice, you are cheating. Okay. So your model, although apparently might beat or overcome other competitors, then it does not really have the abilities that you expect. This unfortunately happens a lot in recent papers, unfortunately, also research, because there is a tendency of just having a table with better numbers than your competitors. Of course, this is normal. I mean, every big company wants to demonstrate that their model that they produce is better with respect to the others. Now it needs a really careful analysis of the paper and of, I don't know, 50 pages of additional material to understand that practically to got those values, they were tuning the parameter on the test set, which is depressing. I mean, it means that the model is not really, it's not really working. It's not really working according to the theory that you expect. Of course, since you have seen trillions of data, then I mean, the trillion plus one sample that you see not be that different than what you have seen in the past. So of course you had a lot of knowledge, but in theory, you should not have trained and fine tuned your model on the test set. Anyway, I mean, the never, when I say never use the test set is this kind of never here. So anyway, we are close to Halloween, so it's a good message. Okay. So now, besides the, besides doing a proper cross validation or adjusting the parameters of the model in the proper way, there are other strategies to avoid overfitting. And one of the strategies, which is called the realization, starts from the concept of the Occam razor. So Occam is, William Occam is this guy here in the slide. And he's just saying that a short explanation is preferred over a long one is always preferred. Of course, I mean, there were lots of other people saying the same before, but we tend to remember this concept as the Occam razor. Okay. So this just the name. Now, so what we are, so where am I going? What I want to say is that you can train a model optimizing the parameters on your data and trying to get to the minimal training area as possible. So you are performing empirical risk minimization, but the empirical risk minimization might be complemented with some extra condition. So the fact that you want to control the complexity of your model. So your model should fit well your data, but should not be too complex, too complicated. And how can we do that? Well, we can just introduce extra condition in the loss. So in the loss, I mean the objective function that we're trying to optimize, we said that everything we are trying to do is to minimize this thing here. So minimize the number of errors that we have on the training set. What we can do is to add something else, which says, yes, I want to minimize the error, but at the same time, I want to minimize the complexity of my model. So, yeah, this is the kind of thing that we can do by adding a regularization term. So regularization term is a complexity term here. This lambda is a coefficient that consider the trade off between the fact that your model has to fit well your data, but at the same time should not be too complex. So you have a parameter that is rebalancing, let's say these two terms. So it's just this trade off. And so we are introducing this extra term. Of course, since we have now this extra parameter, now we can start making the distinction here. This lambda is not really a parameter of the model, it's a hyperparameter. So the parameter of the model is a hyperparameter. So the parameter of the model will be the parameter that I use inside H. So a model would be a line. So I will have W0 plus W1 multiplied by X. So W0 and W1 are the parameters of my model. This lambda is a coefficient that I have to choose anyway, and it is called hyperparameter. And anyway, I have to tune it on the basis of cross validation. So we go through linear regression quickly just to connect with this concept of minimizing the complexity and introducing the extra-terameter validation. So linear regression, we are trying to search for a line that well describes the trend of a set of data. And we need to measure the number of mistakes. Now, in this case, we still want to write down a loss function and to, yeah, this is the expected value of the loss function. Remember, that's what we need to minimize. But it doesn't make sense to use the zero one loss, as in the case that we were looking at before. So this is the loss, right? So we have to minimize this. This is the loss, the number of mistakes that we make. So this was the zero one loss that we mentioned before in the case of binary classification. We were saying, okay, whenever I make a mistake, binary, but it can also be multi-class, whenever I make a mistake, I count one. And so when summing, then I understand how many errors I made. This kind of loss does not work well for regression because in, okay, let's put it the other way around. What's the difference between classification and regression? Anyway, I'm sure you know. Yeah, yeah, yeah, it is different. So one is? Perfect. Okay. So it doesn't make sense to calculate, I mean, to evaluate the error in a discrete way of something which is actually a number. And so it's continuous. So the loss that we prefer to use, and it makes sense to use in case of regression, is the difference between what you predicted and the ground truth. So it's not any more sort of zero one counting the errors, but you count how distant you are with respect to the ground truth, with the ground truth. So the distance between your prediction and the ground truth. Okay. Okay. Of course, this is not the only possible choice for a regression loss, but it's the easiest one. Okay. So this is what we need to do. And yeah, sorry if there is a lot of changing of notation of M, M, but you understand this is the number of samples that we are observing. And we are just taking the average or the sum of all the distances because the loss is the distance. Right. So the errors now is measured in the difference between what I predicted and the ground truth. Okay. So anyway, we have to minimize this and in this way, we are performing an empirical risk minimization task. So we are just using the logic of least squares because then these distances taken squared just to have a positive value. Okay. And here we are supposing since we are running the linear regression that our model is linear. So this is the typical shape of our model. So we need to search for W and B that minimize this loss. So yeah, so this is nothing more than the prediction. Now, I call this class depends on these parameters is an infinite. I mean, this is an infinite family, but we know that according to the discretization tree, we can use that. So yeah, this is what we're trying to do. Right. The data and we're trying to figure a line on the data such that whenever we have a new sample, we are able to assign it to the correct Y position. So we are taking our predicted line and we are calculating the distances between the points and the line. We are taking it squared. We are summing all these elements together. And this is the thing that we're trying to minimize. We're trying to minimize the sum of these distances taken squared. So raise the power of two. Of course, I mean, here is just because we are supposing that our data lives in a one-dimensional world. So X is just one number, but X could be two numbers such that our data live in a two-dimensional space. And then we need to predict the third dimension. And so this is the reason why X is actually bold, as you said, and also W at this point is in bold. It's also a vector of the same dimensionality of X. Okay. So I'm sure that most of you know this, but just to go quickly over that. So this is what we are trying to do when we calculate. So we have a model which is defined by W and B. Right. And which Y is this model predicting? Well, you give X as input, you take the dot product between W and X, you sum B. So this is the Y hat that you are predicting. Then you have Y hat minus Y taken squared, which is what we are trying to minimize. Okay. So now this thing here can be written also in matricial form, just to play a bit with the math. So X, you see here we have N samples. So N training data. Each training sample is composed by multiple elements, a dimensionality of which vector X is dimension D. Then you have Y. So for Y you will have N elements. So one Y for each data. So one label for each data. And B is just a number. W has the same dimensionality of X. You're taking the dot product. And this dot product can also be written in matricial form in this way. So we are considering the matrix X and we are adding in front of the matrix X a column of Y, a column of one, sorry, column of one. And so this is what we're doing. And we are considering W and B together in the same vector. So it's like taking B, taking W and adding in front of W the coefficient B. So this is the vector. And we are considering the product, I mean, matrix vector product. And so it will be a row and column. And of course you can also write this down as just X and write this down as just W, just for simplicity. So this is what you get. Very nice expression, very simple. So you can easily make some math, let's say, with this nice expression. Well, I can skip that. The point is, this is, you can think about it. What is that? This is just an expression where X is taken squared. So its behavior will be a parabola anyway. So we can easily search. Well, in practice, the problem is not in terms of X, it's in terms of W. So you're searching for the parameters W. Now this W contains both W and B. So it's a parabola in terms of the coefficients. And you can always derive this expression to search for the minimal value because you expect this shape of a parabola. What are you doing in practice? You're just taking the derivative with respect to W and putting it equal to zero, nothing more than that. Now this is a nice, as we said, very friendly expression to take derivatives. So this exponent here becomes a coefficient. Then you have to take the derivative of the expression with respect to W, which is practically only X and is taken and transposed due to the matricial nature and matricial form. And then you have the same expression because it's raised to the power of two, the two becomes a coefficient and you will have the exponent minus one. So you still have this expression here. Okay. When is this zero? Well, you have to put this equal to zero, do some steps and you get, this is just a linear system of equation, but you can verify that the solution is this one. This is just ordinary square, nothing more than that. Now this is written in matricial form. And so you see all these X on this shape, but you can rewrite it even in simpler terms as we were looking at before. The form is the same. So yeah, it's just now you are really looking at it in the eyes considering one sample element at a time. So summation of I, X, Y, I, and this is the summation of I, X, Y square. And these, so you can get that W and here we are really, in this case, we are making a simplification. So if when reasoning in matricial form, we were including the B inside W, here we are just instead forgetting completely about B. You can anyway get to this result. And similarly, if you also have the B, so you write it down like this, then what you get, it is the same because in practice it's like having Y minus W zero, which is equal to W one X. So the only thing that changes is that now you have this Y minus W zero in the expression. So you get W one. Let's see here, you have this. And now this Y is Y minus W zero. It's not a big difference. And once you have W one, you re-put it inside and you can get it also W zero. So it's reasonably easy. So now you have, let's see, the way to express your line. And this was obtained by optimizing this part, right? So minimizing this part of the objective function. So the least square errors. But our reasoning was, yes, fine, we can optimize these, but we would like also to have a model that has minimal complexity. So I want to add an extra term in my objective function. And the extra term that I add is as this form. So I decided to write it down like this. And this is my regularization term. What am I doing? I'm actually fitting into the outcome raiser logic because I say that I want my model to minimize the number of errors, but at the same time, my model should not be complex enough. So I'm formulating this logic of not complex, not too much complex. Let me put it like this. By saying that the value, I mean, of my W vector should be such that the norm of this vector is small. Why I'm saying that? Because this expression, so originally we were minimizing only this part. We are minimizing. So we are minimizing the number of errors. The fact that I'm adding also this term here means that I'm minimizing the L2 norm of the vector W. Right? So this is the regularization terms that I add. And we can get practically to the, so we just, at this point, we just have to derivate and put it equal to zero. This is called ridge regression. Since we have also this regularization term. So it's not just some that linear regression. It has this regularization term out there. So it takes a different name, but the way in which I obtain the value of W is analogous of what we saw before. So we can write it down in matricial form. We can take derivative and put the derivative equal to zero. Okay. So this is just the, the, the, the results that we get when we have also this second direct term. We can still write it down. So it's reasonably, reasonably easy. Now, yeah, so this is just to understand what does mean regularization. So we will see that in general, let's, let's recap. So we said that we need our model to be able to make good prediction for the future and to at least try to make sure that this happens. We need to take particular care of how many samples we have and how we tune our models based on the data that we have access to. So first of all, taking care of, pay attention, how we manage the data is related to the concept of performing the cross-validation and the analysis. So that's extremely important and it helps to avoid overfitting. These are acting regularization term is another way to avoid overfitting. Okay. In general, the concept of regularization has multiple kind of, it can be applied in different ways. So these are just two. The acting regularization term is extremely helpful to avoid overfitting, but the logic of regularization can be applied also in different ways. We will see later on when dealing with deep model that we can regularize the model, for instance, by introducing something which is called weight decay. Weight decay is exactly this, is exactly this, is adding an extra term in the objective function that says that I want my model to have parameters whose norm is small. Okay. Now you can do weight decay by applying an L2 loss, I mean, L2 norm, you can apply it on an L1 norm. There are different ways in which you can do that, but anyway, the logic is exactly the same. And another important thing that you can do when dealing with deep model will be also to add the data and to do something which is called data augmentation. So you synthesize data actually to increase the amount of samples that you see, but just introducing a bit of randomness. Introducing randomness is a sort of, is an approach to regularize your model. So we'll see more of them, but this concept of introducing something extra with respect to just the empirical risk minimization that tries to pick the training samples is extremely important to avoid overfitting and in general, to let your model be able to generalize. Okay. So, well, what we saw before was just for linear regression, but you can also try to do regression in case you have polynomial models. I mean, yeah, X will raise the power of one, two, three, and so on. This is still a linear model because it's linear. What we mean, what we need is the linearity in terms of the parameters. So it's that, right? So you don't care about the fact that X is raised to the power of two, three, or N. Anyway, we can practically, if we want to fit a model of this kind, the procedure to do regression will be the same in terms of searching for a model. The only thing that changes is the fact that rather than adding X, now you have phi of X. So it's a more complicated expression, but the model will still be, so it's still linear in terms of W. You still have W that multiplies, I mean, another vector that contains different values and different expression for X, which is now is phi of X. Yeah, the procedure is exactly the same. The only difference is that now you are fitting polynomial rather than fitting a line. Okay, so in practice, here is just a summary of what we've been seeing today. And I think that, by the way, it's also my last slide. And anyway, I would stop here, although we are still 20 minutes, but I mean, I don't think it's needed that we overfill the lecture with lots of stuff. So better that you digest each concept one at a time. So the summary is just saying that we went through the PAC learning theory and it's not, I mean, it's something that you need to know. You need to know what does PAC learning theory means. You need to know what's the difference between the true risk and the bigger risk. You need to know what is a loss function, but of course, this is quite easy. What is underfitting, overfitting. And okay, if you want to provide examples in terms of regression, I will be super happy at the example. So if I ask you what does it mean regularization, please don't get confused in a sense that regularization is a very high level concept. So if I ask about regularization, I want to know the whole story. What I mean is that I don't want an answer that say, okay, regularization means adding a term into the loss such that you make your model less complex. It's fine. But then, but it's not only the only form of regularization that we will see. So this is just a starting point. So this is one practical way in which you can do regularization. We will see more of that later on. This is just the starting point. Now you know what does it mean and what's the concept of regularization. So yeah, I would really stop here for today and we will go ahead then. Okay. Yeah. The bias term, it's not the one that we've been. The bias term you mean is W0? Yeah, it's not the one that we've been. Here, yes, in the matrix form when I discuss this, it is. But then when I restart with the calculation here, it is not. Here is not there anymore. And then I re-add it later on in the later slide. Hi. I want to say that now that I apply for UBSA, the programs and for the fall of 2025. So I talked to some professors for my PhD and they asked me if I have a supervisor or not. So I want to, you know, before the exam I know. I know. I can't. I can't. I don't do it for any series. So, but this does not mean anything in the sense that you can, you're perfectly free to search for a company that already today guarantees a position for you as a master phase student. I don't care. You can do that. So the only difference is that I'm not accepting students in my lab as students for master phases before the exam. So I don't do that. But if there is another company or another lab that can host you and eventually you need me as internal supervisor, then I can do that. Pay attention that case. I'm not involved at all in the science that you do with them. It's just that I want to write a paper and write the proposal by myself and you have you as a supervisor, but I started this semester and we can continue after the exam. But, you know, I can start the web me that I started. Even because I don't accept the students, I define what's the top of the world. So I have some openings, but only for people. What if you take a role exam soon? I won't change anything. I mean, how can you teach? So you're not for my exam to test me. But you already started machine learning. Yeah, I passed the machine learning. The machine learning exam with Yes, but not deep learning. Deep learning are going to start. Actually, I studied last year, but I have a group. I even present the paper. Okay. And I have a group that they don't. So you already attended this course. Yes, I already attended this course. You already attended this course, but you didn't take the exam. No, just the project left. I even study for oral arts. I see. So I can take the oral exam in a few months and just get my thesis. The problem just with the project and the teammates and everything. I understand. Okay. So in theory, you are prepared for me to do the exam, but I understand. So, okay. But can we discuss it? When do we have next lectures? Tuesday? Tuesday, I think. Tuesday and Friday. Because what I want to do is to talk with my colleagues in the lab because anyway, what happens is that even let's make the hypothesis that you do the oral with me and we will discuss and we understand that you are perfectly ready to start working on this. Anyway, we have to assign you a topic, right? And the responsible for this research topic and then to helping you to complete thesis will be one of the person in my lab. So I have to understand who's available at now because usually we wait for a period exam. And if there is someone available now, then we can apply too. Because you know, because I want to apply and the professor will want to perform for me and say, okay, what's the name of the thesis and what's the provider? But it's strange that anyway, someone let you apply for a position. You don't have the title yet. Yes. Yes. Because of that, I mean, You need a provider. Yeah. And I know that this year is my last year. So can you please let me have, do you have a lease of already exam which you passed, right? With the bulls. Can you let me have that? Yes. Okay. So send me an email with your... I emailed you last year, but you don't respond. Yeah, because the logic is the same, right? I was waiting for you completing the exam. Okay. Do it again. Okay. I will check the email now. Okay. I know. I understand that it's Jewish. Okay. Thank you. You're welcome. Thank you. Let me stop here for you.