Okay. So, this is just an example of how to again build a network with convolutional layers and no linearities so the activation function like like the department unit max pooling and then repeating this structure multiple time. Now, in this network you see that we have just pollution activation so pollution which is the linear part of the network. Then we have Ray Lou which is a non linearity we have pooling which helps to compress information, and then towards the end, fully, fully connnected. And we'll see later on that there is another network I think already mentioned which is called Alex net again comes from the name of the author which was Alex with a very complicated surname. So this is the typical kind of network that you might encounter, at least let's say the very first space kind of architecture, so this is called Lynette five is composed just of five layers and is called Lecun. So the lead lay is LecUn. And then there is a network called Ray Lou, which is also a non- linearity.
The batch normalization is a way of normalizing the output of a network. The idea is that it regulates a bit the range of the values and each output of the layer, such that then the following layer again has sort of reasonable range of venues to work with and say both for the following ones. And this helps exactly against this internal covariate shifts so to avoid too much variation of thevalues inside the network. It helps to improve optimization in the sense that allows convergence of the network, to be faster ways or weaker. And so this simple operation is something that we can easily introduce the network without in making the network more heavy or a forcing the extra calculation so it's a very, very easy thing. So this is more usually a small bias that's where they did that to avoid numerical issues. I don't know you but I'm curious. Okay, so this is so we we have convolution. Which is the linear part right so the W that multiplies the X so let's say X is the input to double is our weight and in particular is inside our window that moves right. And then, every following layer is saying as input the previous one, and has to decide progressively the content in terms of W of the following layers, right. This creates an issue of sort of a moving target. So we have to search for the parameters inside theNetwork. The parameters and the range. of these parameters can vary a lot. So you start on something that the following. layer takes the previousOne is input, the range that you can get my very a lot and these then provides the new input for the. following layer.
The goal is to calculate the loss or how many errors the network is doing and taking the derivative of the loss with respect to the other parameters. And this will tell us in which way we have to update the internal parameters and everything is based on bad propagation. And so the gradient flow back right from the end so from the derivative on the loss over the whole network to update progressively the weights. These introducing the better realization improves the gradient flows. In general, it also helps to update again the parameters with higher learning rates. And with various more learning rate you will take various more steps, and this will take forever. If you are to take, quite reasonable, quite large step then maybe you are moving faster toward the minimum minimum and inde able. I mean I mean     acing and we do it per batch of data. It helps to regularize a bit the network so to avoid these overpaid so we can state that better my position also introduce a bit of regularization inside the network to avoid the overpaid. But is that too much in the sense that here, we are really introducing values that are obtained by the calculation right so just taking the average subtracting and divide by the variance, but maybe forcing this kind of behavior by hand is too much. Right. So, if this gamma was the square root of the variance and the beta was E so the average. Well, this why would be practically X back again right so we are doing the set the other direction.
The fact that we are introducing a bit of normalization helps to get. the learning rate without letting the natural diverge right so this is again has this positive effect. As every optimization process that start from an initialization, even the network of course depend on where we start from right. So the fact that you're using better realization helps again to reduce a bit the dependence on the natural foundation utilization because it rebalances the values inside them. And so it acts at the point of realization right so remember when we discuss about the realization we said well through usually on the basis of the outcome razor is a way to introduce some knowledge inside your model and avoid that it gets to a very complicated solution. So, yeah, so now you have also this multiple viewers on the effect of better organization. So where do we put it. Well, usually after a fully connected layer and before the non linear activation function so the idea here I mentioned time age, but it can be relude can be any activation function. And maybe this is the position where you find it, but most people also started moving it around in various places. And it doesn't appear with only the fully connected layers, but with the fully  connected layers as well. And that's why I'm doing this because again, although it's not written here explicitly, but implicitly you are making the assumption that your test data are drawn from the same distribution of your training data. If that does not hold your model will not work.
In the convolutional network is that the crucial part of the network you have still the special information so you have height with number of channels, number of samples, right. And here, what better my decision is doing is performing the normalization to squeeze that say the information with respect to n h and w. So at the end, you'll have one mu and one sigma for every channel. And then you renormalize the channel practically with the videos that you get. So, yeah, this is again, effective better my. decision makes the network easier to train allows for higher rates faster convergence overall the network becomes more robust and station. And this is just a sort of summary. So you can have batch norm layer norm instance norm. You can have a group norm in the sense that you take thenormalization over subgroups of channels and not just one channel at the time. And these different strategies again, overall normalization helps to introduce a bit of a transition side the network, the exact way in which you applied might depend on different tasks and then the type of architecture and doing good. And there are many of them. So they're not all equally valid. Again, depends a bit on what you are dealing with. The most just one nowadays is very low one for most of the architecture. But that might be the problem with some some variabilities. So we know pollution now we have discussed about normalization. We know that the activation functions practically are introducing the non linear part inside the network.
The sigmoid was one of the first no linear function that we introduced. Historically has been very popular, because in theory where it was behaving as the interpretation of the behavior of the neuron. But yeah it's not really perfect it has some drawbacks. So we see that overall the function is not zero centered. So the output will be anyway always positive. And here we are guided by this failure here and the output said that it comes from an activation function, and all the values that it provides are positive. Well, we might be in a complicated situation because these parts for these parts are partial derivatives for the weights. And if this all this ends up to be to the term, it seems that the updates for weights that we can apply are all the components of the updating vector. Which means that only leaving this quadrant an update for an element of the network is possible. So, we can only update for the same direction which means that we have to go in a different direction. And this is not a good thing so you see. And so, at that point we don't have any more information so the writing this is zero so we don’t know what to do, which is not not ideal. And we are using the output of the previous layer to get. the input of the following layer. So that's what we are doing here. And the following step, since we have the derivative of the loss, we have this kind of expression here that depends on the linear part.
The signal function at the beginning actually was used a lot as an activation function but it's not. There's some issues. Maybe we can pass through 10 age, which has similar behavior and see their shape with respect to the, to the sigmoid. But now you see this precious number in the range of minus one one so now it's zero center and we like it, but still it gives the gradients and the saturation region. So, yeah, all this function that has this shape have some positive aspect and some negative ones. The third drawback is that the function contains an exponential here, which, okay, it might be a little bit expensive to compute. But nowadays the GPU is not really true anymore but maybe up a very very small cost. So we have positive value, or even in this case. And we have the usual I mean the same annoyance of having positive values, we can discuss. But anyway, we want to avoid this kind of situation right so mini batches help a bit. It's very computationally efficient because we're practically taking the maximum nothing, not doing very complicated, the exponential different difficult stuff converge much faster. And they also discuss about how biological inclusive is the value of respect to signal but that's yeah, I believe, I detailed. It is really flat and then we know that for every negative value you will have a zero, and it's really zero in the venue, I mean in the network again, because you will not learn anything.
If your network is not working, it's not because of the ReLU. So whatever you have to design a network ReLU is the most obvious choice. There are some variants of the relo activation that rather than having just a ticks the value here they introduce a parameter. So, you can find yourself in a condition which whatever data you provide as input of your data set, you always get a zero as output then yeah your network will not learn anything anymore so you will not get any update. It's doable, but not not easy. Let's say at the end of the day. There's a singularity there's a sort of cuspate in the behavior of the network in zero. So there has been some attempts to avoid that by introducing again back to the exponential function with a loop for the exponential unit. That might be different kind of activation function. We're not really talking about the decimal places in most of the cases. We are looking at the components that we need to form a network. We have different kinds of layers that we have discussed about this with clas clas. So you can squeeze out some small percentage extra, but it'sNot that the reason eventually which you're not for which your network isn't working is not that the problem. Yeah, so general rule of thumb, don't think to art, just use ReLU or a Seleu or something else. Okay, so you can forget all of the ideal choice.
So here's a question about normalization and normalization works for the internal layers of the network. Should we do something also to normalize data, the input part. Well, yes, and it's preferable to pre process a bit of data. There are many things you can do, but again, even in this case the suggestion is to stay simple. So, zero centering the data is useful. And you can also normalize it a bit. Somehow, here, we're using the same logic of better my position but fully for the very initial layer. So just send to the data run zero and normalize them. It can be useful because at the end it has the same effect ofbetter my position in the sense that you're this way you also sure that your data leaves in a nice regular space rather than having very big or by avoiding large variance and very building the themselves in the range in which the data. is. And so, yeah, it's just a way tonormalize inputs. Yeah, and then you can. also this chapter channel deviation which is used nowadays for Resnet, which is what I told you here right in this slide. This is how you perform it when you have images when you do that. It's not common to use some other strategies like PCA or widening or complicates the activation function. Okay, so anoth    s about pulling the various pieces the major of the activationfunction. But there is also something that you need to do on your data.
The problem that we face is the same as also the case of the war. So, considering all the weights equals zero including also the bias inside the convolutional layers is not what you want. It will take practically, it will be very difficult for the network to start updating the weights in the right direction if we start exactly with all the ways that are known. So what can we do. Well, we can think about starting from small values. And here, I'm just opposing to have a matrix of w. I'm thinking about the very simple and fully connected layer here. But anyway you have a dimension ideas input and dimension ideas output right and you have to do w by X. So this is your W. So in the first layer, I have a reasonable distribution almost uniform. So I'm taking this W here from the random distribution, and it's almost flat, but progressively the values that they get inside the network they tend to shrink towards zero. And this is again, not what we want, because if we are in a condition like that. Again, our network is still having all zeros and not knowing which kind of parameter should be updated in which direction. Okay, the gradients are zeros we don't know how to update the parameters anymore. So we're not learning anything and that's not good. So maybe we can do what we can maybe we c    er chapter relates to weight initialization. And within the decision is related exactly to that so what's the initial value that they should assign to every weights in my network.
In 2010 a paper was discussing the effect, but then later on what what has been proposed is what is called this savvy in utilization. And it's nice because again, so we have to be comfortable the name of the person who thought about that. And what's the solution well, we are not setting anymore as your dot your five or zero dot one in front. We're using this we are dividing by one over the square root of the input dimensionality. And these holds this kind of solution with this weights with this value holds for the fully connected layers. If you want to use it for the convolutional layers well you have to take into consideration also the kernel size. So the the input depends on the kernel side on the window side and the number of channels but the logic is the same. So it's a good way toinitialize the distribution for the W inside the network. Okay. So, yeah, that's the survey and initialization. So what happens in the case of which the activation function is not done H that is the value. Practically we have a similar thing so even if we use it it's not good again because we have this peak practically only the railways is flat and then linear over zero. So something that's that's not ideal when we change from time to time is not ideal. But yeah, yeah    an do the same thing but starting from a higher value so zero zero five rather than zero dot zero one zero one.
The choice of the initialization is not, I mean, solved. There are lots of research going ahead. So, we know regularization, we said in multiple time as one typical solution is that of adding an component and element in the loss function so in the objective function. This component might help to regularize that the weights of the network itself. We mentioned, for instance, a better my position level rise a bit, but there are other techniques that you can use to even improve the regularization for your network, because you know that why you start training, you might see that you're the training the cost goes down. Right, so it means that you are going into the right direction, you are updating the weights inside the network. In a way that indeed the performance of your model is increasing. But this is what happens in the on the training. Then if you look at the performance you might. see that the performance in the training is very good and is probably city growing along the epoch of your transition process, but the effect and the validation. also also value of the data that you keep outside your training to check how the performance is behaving. You might see it either remains flat is not improving anymore or starts dropping and then of course you have to overfitting. And that of course depends a lot on how you are initializing your network so yeah they're very sub area of research that deals with the proper how to set the proper initialization for the network, and last years.
Dropout is just is turning off some of the neurons inside the network. So, at every forward pass you just randomly set some neurons to zero so you turn them off. And, and you keep it like that I mean during during the training process. Why this is helpful in general, the, when we introduce, as we said, a bit of randomness inside theNetwork that avoid the network to really memorize the data, then it's good. But we also have to think about how to recover from this noise because we are introducing noise so then how we anyway, and we don't want to be affected negatively by the introduction of this noise. But again, reasoning in this term, it might seem complicated in practice, what do we do. Well, we drop in the forward pass. And then in the test time so this is the during training at test time, we use all the neurons as if the dropout was not there in training, but just reweighting them with the same thing that you use to threshold at training time. This is how the work and the network is working. So we are just. just looking at which neuron we want to drop by setting the threshold and say, okay, that one so it take as many values as the number of neurons, check it they are over a certain threshold, but here is just really it's just assigning random values between zero and one to the to the neurons. And the other half we keep on.
The idea is to average out the effect of the randomness is just taking the network as it is with all the neuron on, but putting in front of this output, the same probability that they used to turn off each of the neuron. So, yeah, that's the practical things that we do. So while inverted dropout, this is simply what what what's applied to avoid extra time and test. And you can understand why this is working right so what we are doing is just moving pieces here and there, but the logic at high level holds and seems to work experimentally so it's something that we can apply. So if you are dealing with classification, you want to augment the data to create new data. So for instance, introducing some noise, you can do it directly on data, right, not necessarily inside the weights and so the activation of the network and introducing a bit of randomness in the input is nothing more than that of mutation so you increase the amount of data. And so we change the appearance of the data at the end, how should you looking at the data well of course it depends on the past, you're dealing with. So we like it. So this is really some practical trick that we're using seems to be working. So yes, at any time. We know that that's normalization has this effect because it introduce a little bit of regularization dropout has that effect. Overall, yeah,. this is the introduction, regularization has different way of appearing inside the network.
You can do color jitter, change the brightness. You can apply PCA reducing the number of colors in the image. Mix up is really used nowadays. They're also very different variants of this mix up that seems to be working nicely. So, you can really go crazy with this kind of process. Next time we'll start discussing the training dynamics and what happens after you should not you should you not. Thank you for your time and good luck with your project. Back to the page you came from. Back from the pageYou can also decide to take crops of the image and training time so you have multiple crop at test time. What you can think about doing is for every image you take four or five crops or four angles and central one, you make prediction for all of them and then you average the prediction. So yeah, just the things that you can do in terms of data augmentation. Yeah, so you can also decision to roll the image stretch gocrazy with that. So you have max pooling with different dimensionalize in the area of the region over which you take the maximum or the average. And so, yeah, so regularization of all comes in different forms we said dropout that's norm that augmentation drop connect. So we're killing neurons before now we are killing connection is what drop connect does. See, of course, be sure to not introduce noise in Terms of labels and semantics. So I don't know if I mixed the 40% cut and 60% dog. I never put myself in a condition of having a 5050 right otherwise really I'm introducing confusion.
The total operation is completed in: 1 minutes and 32.14 seconds