The next lecture will be on November 20th. The idea is that you are going to create Google's lights and put the link of your Google's Lights here, the corresponding space in the column. So they have to be there on the same day of the presentation. So just even five minutes before you present, just upload. That's happening. So in practice, what we saw was everything related to the one-time setup, meaning that now we know how a convolutional network is done. We discussed about regularization, so different kinds of regularization and data processing. As well as batch normalization. And now the idea is to focus on the training dynamics. So I think you already started seeing something during the exercise. So even if you started before the lecture, it's fine. Anyway, it will take a bit of time to digest the information. So practicing is always something good. So we tried to go into the training Dynamics today and also in the after training, so details. So if I'm missing someone, if there is any other issue, just let me know during the break and we'll try to fix any possible issue or the groups. Okay. So this will just turn this page off. And we can go back to the presentation, the lecture. Now, ta ta    We are ta ta we are ta. We are Ta ta. The next lecture is on November 21st. We will be talking about how to train a neural network.
The loss at the beginning goes down and even quite quickly. But then plateau over a certain value, so it remains constant. It tells us that our learning rate is high in the sense that, yes, we are moving towards the minimum, but possibly we found a minimum that is not allowing us to really decrease the loss. So this indicates that the way which we chose the learning rates is not the best one and it's quite high. And instead, I mean, the red curve is exactly what we would like to see. So it's a good learning rate. So the loss goes down progressively and tends to go to very low loss value, right? It's very close to zero. So very small values for the lost. This is exactly how we want to see it. And if you don't know what is the exact value, what do you do? Well, of course, we said it is a hyperparameter. So you can keep your validation set outside and try to see what happens during the training process or to adjust the value. But at the end of the day, you would like, actually, to not just keep one single value that might be wrong, but you just want to try many of them and actually change it over time. So in practice, you might look at the loss, what happens to the loss value. And then maybe there is the point in which you go to something which is reasonable, but maybe you want to get to a minimum.
The issue is not the value of the learning rate that you set. The issue is the schedule for thelearning rate decay that you have chosen. So you should be able to let a network converge to some reasonable loss, small loss, even with a constant learning rate. So if you really see strange behavior in your network is not because you have to change or optimize the learning rates schedule. That might help you to improve the performance of the network, but it's not the key reason why yourNetwork is behaving strangely. Right. So the reason why that is happening is refinement, but that's not a reason why it should not converge. So of course, the basic, I mean, the naive strategy would be to keep it fixed. But there can be other options. So it might depend on a specific kind of problem if one of these learning, one of this schedule is better than the other. But yeah, of course the basic strategy is to keep a fixed learning rate, although I said that it might not be a good choice. Just because sometimes you see there are many things that you can vary inside the network and you might search for the culprit in something is not really the reason for strange behavior. So that's why I'm re-putting it here. It's a hyperparameter. There are some studies  that show some hyperparameters are extremely crucial. Right? So you have. to consider that the learning, the role of the hyper parameter is extremely important.
The role of the learning rate might be different depending on the kind of optimization you are using. If you are exploiting stochastic gradient descent, of course, thelearning rate is very crucial for other kinds of optimization. But the other optimization is somehow the, there is an internal adaptive learning rate mechanism somehow. So the performance on the validation set might tell you that you have to stop. So better to keep the model and in here. But specific hyperparameters, such as the padding, as well as the stride. They are all things that you set by hand to design your network, right? But yeah, I mean, in general, those are considered exactly part of the design, so something on which you don't really spend a lot of time in cross-validating. Because, nowadays, what happens is that you inherit a network that has been designed by someone else, so you rely on architecture. But still, there are some that you still w    that tries to avoid setting manually the learning rates, but they have some theoretical basis, but we will not focus on that. But what you really, Imean, your goal is to have a model that will be able to generalize. And so it is extremely important to have an eye on what the validation, so what's happening on thevalidation set. So if you see that the validation is going down, you just apply this logic of early stopping. So not go on training more because otherwise the validation performance will decrease even more.
The weight decay is the extra regularization term that we add together with the loss function in our objective function. So we want to, of course, we need somehow to cross validate those those values. Typical way, we'll just define a grid of these values. And then we test them, right? So we try them out on the validation set, actually. But rather than, so here, rather than using this sort of hyper parameter grid logic, it would be better to consider ranges of values. So you choose a range rather than taking specific value to the finite grid. So this is better, so what I call random layout, because now we have a range. So yeah, so random layout is preferable with respect to just taking a grid layout. And this is just, I mean, we're looking at some figure that they were testing three different architecture. I think they were just running extensively, sort of running on the weight and learning rate and you see they are just covering lots of points. Of cou cou    ant to tune for your specific problem, which are, for sure, the learning rate, as we discussed on it before, and also something else, for instance, the weight decay. So at least these two are useful hyper parameters, which you can keep your eye. So maybe changing one of the hyper parameters do not have any effect on the model. While changing the other might have a significant effect because there is a bump here. So there's a peak.
The idea is that if you are able to just have an overview on the whole range, rather than just having the grid, we are better able to understand how the function behaves. The color here is related to the loss, I mean, the performance and that's just this heat map here. Okay, so what is the procedure that we have to consider during training? Well, so we set some hyperparameters and in the most simple way possible, let's say that at the beginning, we might also decide to turn off any kind of regularization. So first of all, you can check the initial loss, see if it's producing reasonable value. And then as a second step, you startfitting a small set of samples. So we are not even thinking about regularization or dropout or complicated dropout. You really, what you expect to do is that I pass the network a limited amount of samples, then it should be able to memorize those. So I think it was Facebook, so Meta nowadays. I mean the idea is to really overfit       rse,  I mean, this, I Think It's Meta nowadays, I think It'sMeta nowadays, soMeta nowadays. It's a very simple prediction on certain number of classes. The output of our model, we can, Imean, the prediction that the loss failure is something that should, at the start, respect our expectation. What we expect to have is the logarithm of zero dot one in practice.
At the beginning, just to verify that everything is behaving reasonably, I want to verification that my network is able to work fit a small group of sample. So this means that we expect after a certain number of epochs on the small number of samples. If your loss is not going down, and so in practice, you are not able to overfit so the network is behaving strangely, then there is something not correct. So you start really dissecting, I mean, trying to understand it's a sort of an autopsy, right? So you really try to understand where is the mistake. And so since now you started checking the learning rate, maybe you also want to verify, try it out several possible values to understand if the loss is also, you can make the loss going lower with respect to what you initially saw. So at this point you start extending the period for your training. So if at the beginning for the overfitting, you're just really babysitting the process closely, then instead what you want to do is that you launch some of the experiments with different learning rates, you go out, you take a cup of coffee and wait your model to finish. So now it's not really, Imean, the time starts being a bit longer in terms of validations, so evaluation, but still it's something that you can do reasonably quick. Now, what can go wrong still? Well, if you look at the loss function and you see that at the start is not, and you need lots of iteration before the loss starts going down then possibly the issue is not well-utilized.
In terms of accuracy, this is the ideal kind of performance. So you see that the accuracy is going up on the training set and also on the validation, right? So the fact that there is certain gap between the training performance and the validation is overall healthy. So just verify that you get to a plateau before changing the learning rate. If you see something like that, this tooth, well, possibly you applied learning rates too early. And so remember, you might decide to apply the early stopping that we discussed. There are some, also, I don't know, extra things that you might want to check besides the typical and performance of the loss and so the accuracy, loss going down, accuracy going up for training and validation. So we said that stochastic glaring descent was happening is that you're just updating the parameters. So it's the old one are the new one minus alpha the gradient,right? So this is what you are just doing for the update. So if this ratio is that the weight update and the weight magnitude is that we expect, then the weight decay schedule is going well. Okay. Now, if you really want to consider it in detail, also the choice of the kernel dimension for each of the convolution layer, the padding as well as the stride, they're all hyperparameters. And part of them, indeed, define the network architecture, and then you also have instead the learning rates. Another hyperparameter is also the percentage of dropouts, the probability of dropping one of the neurons as we saw.
In terms of learning rate, we have the most, should we say around the 0.01 ratio. If you see is that a very big value then might be something wrong. So it's just an extra check you might do with that. After training, now your model, your network has been trained, right? And it seems that it's working reasonably well. And then some colleagues come to you and say, okay, now that you have this perfect model, let's participate to this international challenge. Just let's take your model and use it for the competition. And what you should say to your colleagues, yes, we can do that, but it will not be enough to just use my model, even if it's reasonably good. There is also nowadays lots of studies and papers and research on things that are called mixture of experts, which are just mixing multiple models together and doing it in a reasonable way. So now basically what model ensemble is doing is just training multiple independent models. And at this time, you average the results. So there are many ways in which you can average the prediction of multiple networks. There are several things that you can do. You can actually combine the network in the middle and try to create a new architecture or simply take the new architecture. So that's, yeah. Okay, so we are back and we have these, we are at the third point. Now, yeah, one time setup, training dynamics, and then we should pass to what happens after training.
The basic thing that you can do is averaging the results. So just look at the probability output, you average them out, and then the assigned class will be the one with the highest average prediction. And these in general helps, especially if the models are different or they have different architecture, different logic, they are focusing on different, let's say sub features or sub characteristics of the problem. So model ensemble is what we mentioned. And then there's also something else we have to discuss, which is transfer learning. Now, what do we mean with transfer learning? Well, when discussing a network or presenting a network, we know that it is composed of lots of layers, lots of weights. So maybe we can not necessarily like that that that we can use exactly this logic of trans trans logic. Why? Well because it's trans logic of logic. So it's not necessarily, let’s say, that that’d be a bust, we can’t use that logic. But yeah, this is one thing thatyou can do. So this is the after training. So you can really try to, I mean, imagine and be creative in a way in which you set the learning rate schedule, right? And then your model will get to the different minimum. And so you will get different snapshots of your model. And it's just, yeah. this is what's happening here. So rather than just always updating the last value of the parameter vector, you just, you consider the last day w with a weight of, I don't know, 99% like the weights and the remaining 99.5 and the other 50.5.
The logic of transfer learning started to be used a few years after the deep network, let's say, the AI world, is nowadays a standard. It means that we can train a network on a big amount of data. And then we can fine tune only a small part of the network with a limited amount ofData for a new task. So it's not true that we need for every problem a huge amount of training data. We just fine tune it, we just tune some of them in a very small subpart of those of the parameter of the model for our new task, because the model, I mean, the foundation model is there, is frozen. And we transfer the knowledge that is in the foundation models by fine tuning only a subpart for our specific task. And so that's what we do with transfer learning, right? We fine tune the network and now your model is now your task and now you have trained your model. And that's how we use it in AI today. We fine-tune it for your task with eventually a small amount of sample for your problem. We don't touch it anymore. We have trained our model and now we can use it for some other kind of task. That's what transfer learning is about. It's about transferring knowledge from one task to another. We can use that knowledge for other tasks, for example, to train a new classifier for fashion or for other types of tasks. So that's the way we use transfer learning.
In the case of AlexNet, and we will go into the details of the architecture on the next slide, suddenly you're on in the lecture, you will see that you have this convolution, Mike's pooling, and then of course here I'm not rewarding the ReLU, but it's just a basic architecture. So if you have your network like that, which has been trained on ImageNet, what you do is that you keep most of the weights frozen, and you just train practically the last layer for your new task. So you have to change the last layers, so if your new set of classes is C, then you will have an ending vector with C veins, right, that will help you to predict just the amount of classes that you want. And so, yeah, this was some results from old papers where in practice they were showing that, so they can obtain good results even if they have a limited amount of samples. Of course, the more you have the better it is. But it is, so what are we doing here? So here is like, if you are freezing the convolutional part, it means that we already have extracted the features, right? So we have a way to extract features from the data. The kind of representation that I want to use is fixed. And I'm just changing the lastlayer, which is a classifier, which  is practically the same as what you would do with SVM in the classification.
The goal of image retrieval is that, is what you would do when you put an image on Google, right? Well, you take your image, you represent it with a vector, and then you search for close vectors. And then the close vectors correspond to images, and the system returns you, the images corresponding to the nearest neighbor. So two similar images have similar representation in the embedding space. Now, depending on how many data you have, you decide how many layers you want to update. So if you really have a very limited amount of samples, then maybe you need to update only the last layer. If you have already, I mean, you're just starting collecting more samples for your new problem, then you can take the freedom to also adjust more layers, so not just the last one. And yeah, for part that you just want to adjust, just remember to keep a small learning rate, because you don't want to make big jumps. So, it's a adjustment. Yeah, so maybe your network was all your network for instance, but then the focus was the target space, so the data extracted for your data ca  ca   omeone provided me a handcrafted feature. So the idea, what this paper was doing is, okay, let's use this as features, because at the end of the day, if you look here, is producing for every sample, for every image, is produce a vector of dimension 4096. So it's just I'm just providing data to a classifier.
The more you train a network, the better is the architecture, the more you enrich this network with knowledge, and then you can reuse it for different tasks, right? So it will be your pre-trained model, which is what we call nowadays, foundation models, very big, someone trained it for you with lots of data. You just readjust it for, I mean, you find to it for a specific task. And there is another axis which depends on the fact that your downstream task, so the task that you really want to solve, might be different with respect to the pre-training task. So if you have a very limited amount of samples here, so just for instance one, so 10 or 100 sample per class, you will adjust your fine-tuning differently. If you have enough data, maybe you can decide to fine-tune more layers. But then if your dataset in the downstream problem is very different with. respect to what you did during pre- training, and you have very limited. samples, well, here you are a little bit in trouble because you don't know how much you can inherit, right, from thePre-training with respect. to your downstream problem, so let's say that here the advice would be go out and collect more data, right. so at least 100 to 1000 per class. Otherwise it's very different, and it's difficult to transfer knowledge. Well, if you think about whatever you want to think about, whatever you will want to transfer from your model, al.    n be reused also for detection, for segmentation, for other kind of tasks, and we will discuss about them.
Transfer learning is everywhere nowadays, right? So it's just a standard, it's not anymore an exception. And so now, what is the advantage? Well, the advantage is that then, even if you have a limited amount of sample for a new task, you are able to get very good results because you are reusing knowledge from some other task. So you can get good results even with alimited amount of samples and faster. So this is just what is indicated here for these two graphs. Now, just to check before the break that you are still alive and attending the lecture, so what are these jumps? So maybe you are changing the learning rate, so the loss has this step down, and of course, he has thisstep up. Okay, so just this one, then eventually, then we stop for a break. So I think we can just stop here for 15 minutes, and then we start for remaining part of the questions. So sorry, I was just saying that we use that practically we use transfer learning everywhere. But then yeah, I mean, if you might not have enough data, then you learn f    y, thenyou might not need to use fine-tuning to use the model. So they wanted to compare what happens when you start with a randomization or the pre-train model. And they were checking also this, let's say, performance of an extra behavior, so they were just stopping here for a 15 minutes.
Fine tuning is practically always useful, but the issue is that do you really want to fine tune? I mean, considering again, the huge dimension of data set on which this foundation model is trained, by fine tuning for your specific task, of course, you might get an advantage for that specific problem you're focusing on. But you might also at the same time forget a lot of knowledge that your foundation model has. So there's always a trade-off between what you want to achieve by the specific problem, and the fact that you might end up forgetting previous knowledge. Then now we want to focus a bit on some of the architecture. So we're going to want to go a little bit on the structure of the network that somehow made the history of deep models. And nowadays what happens is that there has been this big jump from convolutional network to transformer network and we will cover the transformer during the presentation. So now for today, we see some standard architecture starting from AlexNet. So how is this AlexNet done? First of all, curiosity. So this figure here, actually this divided in two parts. It's like you have two sub blocks of thenetwork. So you see it's partial because it's cut here. But it just provides the idea that the information flowed through two sub parts. And the reason was that at that time, it was impossible to manage all the informational weights with just one GPU and the work splitting the information. And then they also draw it like that. Even nowadays, you can manage everything in one single, let's say GPU.
The number of parameters is this one. Floating point operation. This is counting how many operations we do, which in practice are multiplication and then adding, right, this dot problem, this is what you do. Memory, well, the reasoning is always the same, output elements, how many bytes per element are going to do the calculation. Number of learnable parameters zero, right? Because the pooling is not, you don't have learnable parameter, you're just applying. So in this case, it's just 64. And this is the formula to calculate the dimensionality of the output. And so this is overall 73 million. It's the number of floating point operations. So these are the kind of information you want to focus on when someone asks you, is your network efficient or not,right? So these is the typical number that you see when you discuss the efficiency of a network. So how many Operations are typical thing that you record for your model to indicate that it's efficiency? So this is just a small fraction with respect to the, in terms of calculations, the convolutional layer. So the pooled layer is really you can disregard the number. of floating points of the floating point. Okay, now you know how the reasoning works. So now you have convolution, then you have another convolution. So you have poo poo, another poo. And then you just can calculate how many kilobytes you have in memory that you have to store as the new input.
The Alex net has three fully connected layers. So fc6, fc7, and fc8. So six, because you have five layers before each other, convolutional one. And at the end, we just decided to have our W matrix, which are 1000 rows, such that the output is 1000 values, because that was the challenge we said, right? So the image net challenge, we wanted to classify 1000 classes. So this is the amount of elements you have as output. So how do we design a network? So yeah, well, the people who created this architecture, how did they do that? Well, really trial and error. It took very long time to design the architecture. So even though this was just designed at the time, and there, let's say in this part, still, a posteriori, still a logic, I mean, on the memory flop on the flop, wel  wel    ling, another convolution. So here you see, overall, you don't need to have pooling everywhere after a convolution, it might depend on the architecture,. And then we pass to the fully connected part of the network. And this will be what happens here, it was the end of theNetwork. And then they won the challenge through their model. So the challenge was just a way to test out their model, and then they then won the competition. So they won. So there at that time, there was no rule.
Google has been taking part in the E-Imagine challenge since 2010. In the first two years, the winners were shallow models, mainly super vector machine models with handcrafted features. In 2012, the winner was this deep network to the AlexNet composed of these eight liars. In 2013, the network that won was still composed by of eight layers. And so 2014, actually here is two networks, and they are usually both sided because, so first of all, they have more layers with respect to the previous years. They had 22 and 19, they are both the first and the second place. The one, VGG, is a group from Oxford Research, it's not Google, not a big company, but a lab i.e. i.g. Meta Research. And the one, JF, is the second group, and it's a geometric group from Meta Research, i. g. Google. And they are the first, so I mean, we have Google participating to the challenge, and then a second group from VGG is the one that actually won. So this is just to tell you some, let's say, behind the scene, because this kind of value anyway seems low, but indeed it's top five. Okay. So, yeah, so the error rate dropped a lot, right, so it was really, really good performance by the time. But yeah, again, just more trial and less error, right. But there was no exact rule of how to design the architecture.
The Alex net and the Google net are two variants of the VGG with 16 layers and 19 layers. In Alex net, so they were using, for instance, this five by five convolution. Instead, in the case of VGG, they are using something different. So in practice, they were pooling is reducing spatially, right? The output of the previous layer, you have a certain spatial dimensionality. So you pool, so you reduce in terms of spatial, but then in terms. of channel, you want to double. So these maintain the volume of the network, the number of parameters that you have more or less always the same along the work. But then at the end, the structure is always the. same. They have three fully connected layers than the self-management output. So they are repeated blocks with this structure. And then the fourth and the fifth, then depending on the V GG 16 or 19, then they are just adding one extra cone. Yeah. And these, so just forget that this first part is about starting from here. So think about about about we are starting from the part part part one. So    n Oxford that they produce very good results. So it makes sense to just consider both of them. Okay, so let's consider VGG first, and then we also look at the other network, which is the GoogleNet. So the network is, now we start having some logic. So if before it was just a try and error, then so the group, the VGA group, what they try to do is try to understand which kind of rule we should consider to design an architecture.
The VGG network is much more costly than the Alex net. It has more non-linearity, and a lot of non- linearity, so it can fit very complicated functions. It also has a small advantage in terms of memory, but you keep overall the volume equal. The Alex net was complicated, they had to decouple it in two parts, so how did they train the VGG? Well, they were smart, so they were practically training subpart of the network, just initial layers. They were putting the loss at the beginning, so just few layers, right, and then they were classifying the model. And then they tuned it out, so actually train it iteratively by adding more and more layer, so yeah, they also had a smart way to all through training network, or learning network, and so with VGG 19, this was the performance with V GG 19, and in in VGG and in V GG and in Alex net, the blue was the AlexNet, VGG is extremely costly, and    in terms of parameters, fully connected, it has extremely costly,. much more the Alex Net, and also for the number of floating point operation. It's just a different strategy, but it's just the kind of situation you are dealing with, so you have, yeah, this is the kind to be dealing with. It was used just to for illustration, and than what else, well, all max pooling then regarding this choice of the max pooler.
Google net is a network that was created by Google. The network was designed to be as light as possible, but also as deep as possible. It was named inception net after Leonardo DiCaprio, who said we need to go deeper. Google net is much lighter with respect to what you would do in the case of a VGG, so this choice was done on purpose. It's also much more complicated than VGG in terms of the number of floating point operation and also the parameters, so that it's much lighter. It also has an aggressive stem, so the initial part of the network was reducing the inputs a lot, so they were shrinking a lot the dimensionality of the input, so here in practice they start from 224, so 224 by 224 by 3, and at the end of this stem of this initial part, they end up with a tensor that is 28 by 28, so they were really squeezing a lot of thedimensionality, so yeah, I mean the logic here that the one of the, there are many novelties inside this architecture, so one of these is the this module, the idea of using more convolution together, but then there are also others characteristics like, Imean related to the input. It is a very complex network, so we'll just use, many of them, before entering into the more complicated and complicated  convolution before you'll have the more complex  classifications of the classes in 2014, and what about Google net?
GoogleNet and VGG had a different way of flattening the information through global average pooling. This is a way of squeezing the amount of channels to before entering this more complicated convolution. VGG added extra loss to help the network to converge, so they just add extra loss, so it's just an extra output. This helps to regulate a little the gradient flow, and this helps to control better the way in which to update the parameter of the network. These kind of tricks are things that teach you what are the important part, I mean, how to structure a network, right, so expect me to ask these kind of things at the exam, so don't just go over it and forget about it, so how how GoogleNet is structured is typical of the InceptionNet, what the authors are trying to introduce to obtain better result. The Inception net was created by Alex Netted as fc6, fc7, and fc8, and they had three fully connected layers for instance, where the way is in just one fully connected layer, so what is the fullyconnected layer doing? You remember we start from convolutional parts, so you inherit the outer part of the Convolutional part which is tensor with a special dimensionality, and then before entering the fully connected part you have to just enroll, enroll let's say the information to a very long vector, so  you have to flatten the information, so in practice this is what you get here.
ResNet was introduced in 2015, and you see there's a further drop in the error rate, and the amount of layer is increasing a lot, so we are passing from 10, I mean, order of magnitude 10, let's say, to 100, so revolution in depth in practice. ResNet observed a strange behavior, so in practice, they were looking at this kind of results in terms of test error. The network is not able to decide that some of the layers should behave like the identity function, so they don't do anything, so there's not a problem with the optimization, but in the way in which the optimization problem is faced, so what we would like to have is that a deeper model, so a network with a lot of layers should be able, at least to do emulate a shallower network, right, they should be, so with 56 layer, we would be able to produce at least the same results of the 20 layers, so why does not happen, you know, so this is not happening because the network was notable to behave. For the product, so to facilitate somehow the ability of the network to behave like identity matrices, we need to facilitate some shortcuts, so it should let the network understand that maybe some blocks are not needed, and just skip them and so they're not used, so again, the network shopped and used, and so on, and this is like saying that these are not like turning them completely off.
The resnet was also exploiting some of the ideas of the inception, so they also have an aggressive stem as well as google net. The top five error was reduced with respect to the previous results, and then they could also go deeper, so resnet 18, resnet 34, even lower results in terms of error. Resnet is I think one of the networks that practically was used in most of your projects, so it's really the sun there nowadays, it's true again that there has been this big shift towards transformer, it also true that there are resnext, combnext, there are many other variants of residual network that for, I mean some applications have shown to be as good as the transformer which are more complicated. Now this is really one solid reference that you should consider for any project you want to start from, starting from rnet to VGGNet to V GGNet. Now starting from Alex rnet, this is the overall overview of the overall architecture of the resnet. rnet is the network resnet, where they have these 18 blocks and these multiple stages, so yeah, so multiple stages in one ofThe blocks, and so you see again, so the top fiveerror was reduced, but with resnet34, really the results was good and better, and you see the basic block where each inside each of these group, inside the skip connection, you can either consider this kind of structure here, or introduce a bollard on it.
Inception before is very, I mean very good performance, but yeah if we look then VGG we just discussed that very costly. ResNet is moderate efficiency and very high accuracy, so it's a good reference point, yeah. narrow architecture search, it's an interesting area of research that tells you that, so try to define again, high level rule or logic of how to design a new architecture. If you let your network decide by itself which architecture should have, then you can get very very good results, okay, so much better with respect to ResNet or any kind of architecture that you design by hand, but of course this, you have this meta optimization that it's extremely costly, so yeah I think that I would stop here for today and then next time we go ahead and look a bit of the attention, Hello, so you to wait, just let me turn this off,     esnet, and then in the years there has been a lot of also variants of the inception, so inception two, inception three, four and so on, so top one accuracy in the image net challenge, it was going upper and upper, so here in this case we're really looking at the correct topOne accuracy, and but of of course you have to always consider a trade-off between the performance but also the number of parameters and yeah number of operations, so this is a kind of plot that tries to summarize the accuracy, theNumber of operations.
The total operation is completed in: 2 minutes and 36.84 seconds