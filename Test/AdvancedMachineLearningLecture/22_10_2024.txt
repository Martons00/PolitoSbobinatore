We're going to start back to where we left last time. And it was late and we were looking at computational graphs and our eyes were just crossing because it was too much. So better to restart on there and try to understand what we were talking about. And so I get back to this light. And now I just uploaded a new part of the slides and we'll also discuss them later on today. Okay. Let me check if everything is working well on the zoom class. I think so. People from home can hear me right. Great. So we are set up to to start. We're just doing this simple exercise of defining a graph for general function so here we are talking about a very simple function dealing with scholars so take a scholar's input produces a scholar as output. So indeed this is an extreme simplification, but still, it's a way to go through the process. So, we just do a sort of exercise and we try to describe something which is called a computational graph. So let's say that we have this simple function here, and we put ourselves in a computational mindset. What would be our, I mean to put this function inside a program? Well, we need to find some pieces inside this function we can reuse, and then we co- co-opt them. Okay, so we start back from training. So what does it mean to train and work in practice? As we said, we are dealing with very complicated functions, which are combination of linear parts and non linear parts.
The loss function is a function that measures how many errors we are doing and provides us with a scholar. So when you think about the loss function again, it's just about, multiple inputs and one output. So you will see that it is what  it is indeed in this process in the deep network. So, the graph is constructed indeed programmatically so we are combining you see one function inside the other so we start from X and then we reuse the function so we use the function inside another function. So it's a very simple example, but it can become extremely complicated but again it'sjust a way of composing a function of pieces through pieces that then we can reuse. Right. So here we're just playing a bit around with a generic f of X. Now, the loss. So the target will have lots of inputs, and just one output so here this just figurative, let's say, a scheme of a general computational graph. So we can, if we have a real value for X, we are just providing it as input, and then the corresponding value that is calculated you can we can store it in the subsequent node. And then these notes can be reused multiple times. Now. The graph is slightly more complicated, but the logic is the same of what we were doing before. So now we can take then are divided by why, which is this piece here, but this piece. We also have to consider that are here should be raised to the power of three so we have this term. Then we can put them to pieces together to get it.
The fact that the output is just a single dimension is the things that then guide the choice of back propagation rather than forward propagation we will get there. So we can consider the derivative of each of the nodes in general of this graph with respect to the input. And we just were writing down all the corresponding derivatives one per line, even starting from the more trivial one. So, at the end of the day, the number of operation that we're doing here is practically the same of what we need to compute the elements inside the graph. We are really focusing on that we really are really interested in a function that goes from a scholar to a vector. A typical loss takes something which is possibly living in a vector, which is a typical loss for a scholar. We will get to that in a few minutes, but first we have to look at how to construct the function and then on the basis of the graph, we also writing down how to define the derivatives of this function. We have to do this in the same way as for computing the elements of the computational graph, so define X, define Y, define Z, exactly as for the element of the function of computational graph. And then we just calculate the corresponding one and the amount of steps that we need is the same as the calculation will be the same. So in practice, we are just taking into consideration of the fact that we have two parts to get to the same result and to the result of total result. This is extremely simple.
The calculation of the derivative is not that easy as in this hypothesis here. We don't have to calculate only derivative we are actually dealing with partial derivatives. So, even if apparently we found a structure a strategy a computational procedure to obtain to have an efficient way to get to a result when calculating the riveted complicated function of functions. Indeed, it is not really our case so when we go to apply it in a specific case in which we are interested in support the case of the loss. Still, it's too costly. So we need to find another way or at least to hack our way through this problem to be able to calculate the derivatives. We are trying to define this procedure as a sort of automatic differentiation in forward mode. And we are exploiting the chain rule of course because it's it's just the idea of having function of function offunction so inside is this we're exploiting this tool mathematical tool which is theChain rule. Now note that this was something that they already mentioned last time. So here, we're doing something different so automatic differentiation and then the tool that you will use, you will see this called auto diff. And so that's the pieces then in the computational process that you'll use for your exercise for your project and so on when dealing with models. And now is the real deal with respect to X and just we have this expression so this is calculating the derivative in the other direction. Now that the final effect should be the same, so that the derivative at the end, the results should be same right.
In the previous slide we discussed the computational graph. We discussed how to go from X to Y from Y to Z and so on right and that was just the forward mode, calculating the elements that need that you need to compute at them. So, then we saw the. forward mode in terms of differentiation and the derivative, and then we went to the reverse. mode for differentiation. And this is practically the secret strategy that allow us to calculate the parameter of a network through the derivatives that we need so to optimize the. parameter of the networks. And of course, these are using parentheses of using parentheses to indicate how the Jacobians are accumulated. So this is what we intend with the reverse mode automatic differentiation of backward mode out of the you will see again, appearing this in the exercise session later on. So we need to calculate what is called a Jacobian, which is a matrix of gradients. So the number of operation. We are dealing with has this shape. And while if we do it in the reverse modes, the dependency will be of just one. And so, this is the reason why calculating the derivative in the other direction provides us the best strategy. And we are going deeper and deeper inside the network till when we can get to the output. So that's what we are doing here. And the problem is dealing with the loss function right. A loss function that has a lot of values here as input. But if you do the other way around, your calculation will be easier because practically you're dealing with one baby.
In practice, the least of Jacobians that you see here is the same of the one that you sees here, but the procedure for calculating it, allow us to have a much, let's say lighter strategy with respect to doing it in forward mode. So, this logic of taking the derivative in this backward mode in practice is what saves us somehow to have an easy way to get to have our life a little bit easier in being able to optimize very complicated function now. Back propagation is, as we said, the core element we're focusing on at the moment. The chain rule so back location is not just the chain rule theChain rule is a mathematical tool that we are using. And indeed as we mentioned last time is something that he didn't introduce I don't think it's exactly the thing why he now want is really the Nobel Prize, because there was other thing that he developed, but this isSomething that for sure came from him. And in general, searching for a global minimum is not said to be the best thing because global optime sometimes is just telling us that we're not competing because we are maybe obtaining zero loss so perfect, perfectly matching and predicting correctly every single training sample, this is not what we want in terms of a mode. But still, we are quite satisfied by the fact that we can go towards the direction through the graduate descent right we discussed last time. We are good we are able to go to the good direction that minimizes the loss so we are descending the loss landscape.
The lecture was dedicated to something which is called babysitting the learning process. So you have to look at what's happening, while your model is training, observing how the loss is changing, such that you can realize if there are some issues in the training process. It does not exist one single way to optimally train a model so it's not a soft problem training a network is still an open problem with lots of research going on. So we abandoned for a while maths to go a little bit more into concepts. Let me just get out of this light so while I'm changing the deck of rights if you have question just raise your hand. This. Okay, so. This is a very simple neural network, and I'm calling it very simple, because we have notes, and all the notes are connected among each other. So, we have weights. The weights are the importance of each of these connections. And the parameters of our network are all these weights. And this function here, which is just maximizing sticking the maximum between zero and the input. And then we have an output. So our network, our deep neural network is just a composition of these multiple layers. We have no linearities inside. And here we have been discussing about the sigmoid function, right, so the, the shape the function, but we also mentioned already some other kinds of nonlinearity, like the value. So the TTA that we have there is actually the weights w here.
The fact that our network network is modular is something that we actually chose on purpose, because we wanted to have a kind of function that was as universal as possible. So, and, and moreover, again, it's not just a matter of minimizing the error on the training sets, but we want our model to be able to generalize. We need to introduce somehow some prior that guides our model and that tries to help us to again simplify our life. And the prior depends on the kind of task and kind of problem that we want to solve. Yeah, so, where am I going where I'm going towards the direction, the fact that yeah we're trying to build a model, but this model should serve right to solve some tasks. The task anyway will be solved starting from the data. Now, this is not only the case of images, it holds also for text. Right, so it's a very genetic kind of discussion that we are going to do. And indeed, many of the things I will present you is something. that can be reduced for some at least some sub part of tabular data and some sub parts of text data so but the reasoning is similar. Just let me add one, one thing is true. The origin of. the model could work was thanks to images, right. So when there was this revolution of AI, passing from the old shallow models and super vector machine to the trust into the model was because the image net demonstrated that the model will work.
Images are data that have repeating patterns that are structured that are inside an image tend to be repeated and there is a logic. So, yeah, so we know that there are repeating patterns in the images so now our brain, for instance, by looking at this image already knows that the which pieces goes together. So that's the existence of this structure is something important. Another thing is self similarity which actually is connected. So here there is clearly smoothness and smoothness is a structural prior in our data is information that we can try to exploit. And again, we can demonstrate that is there. Because again, if we take an image we remove a piece. For instance, in this case we're just removing the ego from the from the image, and we remain with this whole. Well, we will be able to recompose the missing part, just by using remaining pieces of the image. So again, it's clear that inside images we have this natural self similarity that we could exploit. There is another property, especially when we want to focus them on the semantics of the data content, which is translation invariance. So the information that isthere in an image, this image practically is the same. Right. And we will say that if we have to label this image, we would say cut, regardless of the fact that isn't on the bottom right corner or on top part of the images we don't care. The label is theSame. The Label is the Same.
Images tend to be compositional in the sense that the images are composed of small pieces but then these small pieces can be combined together in larger and larger structures. So for instance, you can compose these pieces to create an eye, but then there will not be that much different between the left and the right eye. So this logic of composition is another level of kind of information that we exploit to build our network. Indeed, the what I'm moving is in the direction of what we call convolutional network. We might make our life easier and the network lighter, reducing the number of connection. And we can then simplify our life because we have weight sharing. What I mean with weight sharing is that these three weights or these three connections. So we said that what we are optimizing in our network is netwo are netwo   Â lly the same. So the y of the transformed F is equal to the wild. The label of F is the same regardless of the fact that I applied a sort of transformation of the image and here is a very simple translation, right. So, and these are the building blocks of any image like this corner blocks of color shades. So what are we doing here, we are applying this concept of convolution and in particular, we call it Convolutional layer. Right? So this is very high level. It's a geometric concepts, but. then they are combined together and the struck the start building larger pieces of things that anyway will find an image.
The concept of convolution is actually something that we are inheriting from signal theory. So we will inherit this concept for our network. And you will see that this term will reappear also later on for our neural networks. We have a kernel, which is actually what is, and we have a function of the kernel which is convolving the function and the result is something that is called feature map. And in particular, we're exploiting weight sharing, meaning that the weights that are assigned to these three elements here. So once we have this simplified representation. So this simplified structure, we have much less connections. So, this is just how the convolution can be seen as a operator, right? So yea, yea. So you will this this concept will be formalize. You will see now what I mean. So there will be more clear explanation the next slides, but we will have in practice just as a high level concept, a window that moves. And this window will keep the same weights. So I need to search for these weights and then maybe I will choose also another window and another pass on the network. Okay. Then, yeah. This is just to tell you where this logic of Convolution comes from. So it's really the tool that we can exploit to simplify our life and use this logic as a sort of prior to guide the structural learning approach. And again, you know, they are. What we are saying here is that this mathematical tool of convolved has some properties that we like, and that again fits well in the story of how our data is structured.
The logic of convolution holds not only for continuous function, but holds also for discrete function. We are just taking a window which is moving over the image and is performing a convolution. How this convolution is executed by just taking the product of what is inside this filter with respect to the image. So the filter will just move. Okay, and this is how convolution works for image data. Now, and where is all this parameters we have been talking about for hours about parameters so now, where is the parameter here? Any guess? Exactly. So, the value, the content of the window. What we call kernel, what we call filter. So now this small red window will contain numbers. And these are the numbers will be the parameters of our network. And so we need to search for this W but it's just simple withrespect to having a fully connected layer, fully connected network. Right. So we will also add something else inside the network so we'll not only have convolutional layers, but we'll also have some intermediate other kinds of layers so here I'm putting as an example that the pooling layer that will further reduce the amount of parameters that we need. Why we need that, because we want to impose the that sort of zoom out effect that we were looking at when we were observing the fact that in an image, we have compositionality right so we have. First we look at small edges and blobs of color, then these edges and Blobs of Color are combined into something which becomes a structure pattern, and then this pattern is combined to look at more semantic shapes.
So, as a network is trained, we can look at this and we see that they have this shape. So what's happening here in practice. So, these are the filters that the network found out while being optimized. And the effect of applying this filter on the input is that then we have some subpart of the image that is highlighted. And so we see this effect of highlights, or, yeah, region in which we have this, I don't know, horizontal region of color. So it's just saying that we are looking at part of the images in which. We have this pattern. So these is uniform and so this part of. the images highlighted because it's reacting to this kind of filter and so on. So this is what the network so and indeed these patterns, let's say let the input respond in this way and so we. see that this will be the output of the dog product of the product of w with the input. Okay. Well, we also now understand what do we mean with which sharing because we are just reusing the same filter multiple times over the image. So yeah, so at the beginning you just have this. We are taking just a support of this. Okay, so the first three elements here. So there's a part of this filter that remains outside one column and one row of this remains outside. No, sorry, just one. The same filter that is multiplied over. the image with this moving window, and we take the output and we show them here so we are reusing same ways in multiple places to see the output.
The network is trained on image net so the big data that contain 1000 object categories. Once this is trained, you look at the filters. The filters world very first layers, they were just capturing the basic structures. And then when you go inside with bits more inside the network in the middle layers you see that they were capturing structures, and then towards the end they are struct capturing shapes. So this is how we use this logic of filtering and convolution over the input, and we get the output that we call feature map. Then, usually, once we have done this, we also apply something else, and I was mentioning it before,  and we call it pulling, right, some pulling layer. In pulling, we can have different kind of strategy. The simplest one is max pulling, which means that we are taking the maximum out of a certain region of the feature map, so we might have a new window. So we decide even for the pulling, We will need to decide the dimension of the window that we need to look at. In this case, we are looking at regions of two by two. So for instance, in this region, we have 13, we put it there. And you see the effect of max pulling is reducing the dimensional idea of the Feature Map. So the effect that we have is that we're progressively zooming out from the image and looking only at subparts of the image itself and try to build the fine information of larger receptive field.
The Alex net is a compositional multiple layers. We have multiple layers so it's really what represents compositionality the filters. It's what we use to extract features out of the image and learn the kind of feature that we want to construct. The convolutional part works as feature instructor. So you learn how to represent your data. And so the final part will work as a classifier regressor depending on the task that we are trying to solve. So all this structure allows to get the effect of reducing the number of partners of the network. So again, we are making our life easier in designing the network thanks to all these elements that we're exploiting. So do we want a break? It's 530 and then we start 15 minutes. Thank you, thank you, Thank you. thank you. Thank. you,thank you.Thank you.thank you,Thank you, thanks you, and thank you for the questions. Thank, thank, Thank,thank, thank. thank, thank and thank, thanks, thank-you, thankYou, thankyou,thank. you and thank. you all for the time. Thanks, guys, and good luck with the rest of the year. Back to the show. Back on the stage, we'll take a break for a short break and then go on to the next part of the show, which is the next segment, which will be about the next generation of the Alex net. Back onto the stage.
The convolution process instead is different we said that we have these filters. So, we have an image we keep its structure so we don't have to flatten it out in a very long factor. We preserve the structure, and then we take a filter of dimension of the five by five. Now, of course, the filter should have the same depth of the input right otherwise you cannot take the product. So what we do, we take the filter, we move it over the image. And we produce one number is a dog product and really every element is multiplied together and then you just sum up all the elements you get one thing. And this is again the convolution corresponds to the linear part. You remember, our function is composed of linear function that then goes through a nonlinearity that another linear function and so on. And in practice you can see the now is six right with six filters you get an output which is now composed of six and depth. In the next day here, restart between the logic again, and this will be the input of 28 by 28 by six, and the next. day here then restart between here and here again. So yeah, so    the logic is just linear multiplication indeed. These parts here so in this expression of this linear expression. W is our filter. X is the input is the image, and anyway you get, I mean here the depth of course is one because every time you got one number right.
The convolution layer works to multiple filters. Each of these filter has a bias, right so it's a w, which is the content of the filter and be which is outside. The number of filters that you apply will design the fine, the dimensionality so the depth of the output. Then you can decide to apply 10 filters in the following step. And so the output will be 10, the death will be ten, and you see here that progressively here we are not. applying any pulling but progressively the nationality of the out the activation of the. output is squeezing a bit so 32 by 32 and now it's 28 by 28, then it becomes 24 by 24, and so on and so forth. So, yeah, this is what we're doing that we have convolution and then the non linearity. This is the effect, thecontent of these filters. You you you you. Okay. Hopefully to stay. Okay, good. Yeah. Yeah, I don't know what happened. Let's try to get back there. okay, sorry for the people at home for some reason zoom died, but we just stopped and then we restart. So this is how the convolutionlayer works to several filters. Okay? So, usually we have a value and also the rellows so we already mentioned some of them. The most use one is to today's the relows so the rectifier unit. And this is just your product of pollution, and then you can also apply therellow and you get this the output so the reillow.
The receptive field practically is the area that originate that gave origin so the source of the information that is really the source. of every single element in your output. So, every time you take the product of the filter with the input, you get one baby. The receptive field from one layer to the output to the other is just the area of the of the kernel. But if you think about, I mean in long terms of multiple years, then the receptive field of these elements here is practically the whole image. And so you can use that to understand was the original dimension, depending on the number of layer, the area. that provided the origin. So this is going on and on continuously. . . This is going to be a very long talk. Okay. So we want to change the dimensionality of the death. So for this, we use a convolutional filter of dimension one by one. And of course, it will be one by 64 because every filter has to be one. Since the depth of the output is the same as the depth, the depth. of the depth then the output will be the same in terms of depth. And you use 32 of them. Right? So, yeah, you can play it out with the with the formula that we saw before. Of course, you have to remember that if originally it was seven by seven. So what you are doing is that you are practically, this is becoming. two rows and two columns right. So though you have a frame which is practically done by just one set of pixels.
The idea of using the one by one convolution can be extremely useful because it reduces then the dimensionality of the layers of the activation maps actually. So we'll see also pulling, but this is also another thing that we can use. And indeed it is exploited in one of the bottlenecks that we will see later on for specific architecture. Okay. So the output we said in the previous slide here that the output is after this operation is 32 by 32 by 10. Every single element of this output was the result of a multiplication, right, it's just a multiplication filter with the input and then the summation of all the elements. So overall, every filter is composed by 76 parameters, and we have 10 of them. So that's the total amount of parameters that we are searching for, for this layer. Right, and this grows, of course, this is just from one day. So you see how huge is the number of calculation that the, that we need for solving an effort so optimizing an effort. Okay, so how should we reason, or should we reasons from the output. Let's start from theoutput that we have. We have originally our input is32 by 32, right. So every filter has dimension five by five by three. And we have a dimensional the filter which is K, F, s, P and zero is the amount of padding and now you have a explicit way of writing the formulas. So this is a sort of summary of all that we saw before.
The pulling layer do not have learning parameters. There's no parameters that your network will need to learn. No W is just an operation that to the size is the beginning can be taking the average, or taking the maximum. Still, there are some hyper parameters. I mean, even the dimension, the oldest number here dimensionality of SP we said is up to the designer, they are hyper parameters that you can decide in the case of the pooling. So usually you don't apply padding on the pulling right so there's no they're not adding extra zeros around the image before. So you can just don't use it and this is just the logic of the result and function that you get. Okay. So let's say that we and go ahead next time next time. We'll have as many values as the number of classes that you want to predict. And the value that you will get, we will present the probability of a certain class. And so this is what allow you to get the prediction. Okay, so how do we stop here? We stop here. We're tired are you tired? Okay. Next time we'll have a summary of everything we saw before so just, you can keep it. And these are some of them. And we'll be back next time with the rest of the story. And this is the final effect. And then that's the point to introduce the pulling so the pulling is not necessarily does not have to be always at every calm and it can be just parts inside the network.
The total operation is completed in: 2 minutes and 5.88 seconds