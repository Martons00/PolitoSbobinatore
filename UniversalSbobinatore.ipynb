{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Bx5rBfgTUBW"
      },
      "source": [
        "#UniversalSbobinatore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WprGY38WTXvV"
      },
      "source": [
        "### Installation of Dependencies and Environment Setup\n",
        "In the next block, we will install the necessary dependencies and set up the environment to run the subsequent code. This includes installing libraries such as Whisper, MoviePy, Transformers, Selenium, and others, as well as configuring the Chromium browser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuAaK2kH6SZZ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "total_time = 0\n",
        "start_time = time.time()\n",
        "\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg\n",
        "!pip install moviepy\n",
        "!pip install --upgrade transformers torch\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install selenium webdriver_manager requests\n",
        "!apt-get update -y\n",
        "!apt-get install -y chromium-browser\n",
        "!apt-get install -y chromedriver\n",
        "!apt install chromium-chromedriver\n",
        "\n",
        "end_time = time.time()\n",
        "total_time = total_time + end_time - start_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9qL02qCVHHa"
      },
      "source": [
        "# Extracting Audio from Video and Transcription with Whisper\n",
        "\n",
        "This script uses `moviepy` to extract audio from a video file and save it as an MP3 file, then uses OpenAI's Whisper model for potential transcription. The process includes:\n",
        "\n",
        "1. Loading the video file in MP4 format.\n",
        "2. Extracting the audio and saving it as an MP3 file.\n",
        "3. Optionally, using Whisper for audio transcription (not included in this snippet but could be added).\n",
        "   \n",
        "The result is an MP3 file that contains the audio from the video, ready for further processing or transcription.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIVno5is6xJx"
      },
      "outputs": [],
      "source": [
        "# Path to the MP4 file\n",
        "input_file = \"\" # NAME OF YOUR VIDEO\n",
        "from moviepy.editor import *\n",
        "import whisper\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Load the Whisper model\n",
        "model = whisper.load_model(\"small\")  # Load the model on the appropriate device\n",
        "\n",
        "# Name of the output MP3 file\n",
        "output_file = \"audio.mp3\"\n",
        "\n",
        "# Load the video\n",
        "video = VideoFileClip(input_file)\n",
        "\n",
        "# Extract the audio and save it as MP3\n",
        "video.audio.write_audiofile(output_file)\n",
        "\n",
        "print(f\"Conversion completed! File saved as: {output_file}\")\n",
        "\n",
        "# End the timer and calculate the total time\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "print(f\"Total operation time: {total_time:.4f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_STGCwuqVeL8"
      },
      "source": [
        "# Audio Transcription with Whisper\n",
        "\n",
        "This script uses OpenAI's Whisper model to transcribe audio from an MP3 file into text. The process includes:\n",
        "\n",
        "1. Loading the audio file in MP3 format.\n",
        "2. Transcribing the audio using the Whisper model.\n",
        "3. Saving the transcription as a text file.\n",
        "\n",
        "The result is a text file containing the full transcription of the audio, ready for review or further processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRcsGt4x6eDT"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "# Path to the MP3 file\n",
        "input_audio = \"audio.mp3\"\n",
        "# Name of the output file\n",
        "output_text = \"trascription.txt\"\n",
        "\n",
        "# Transcribe the audio\n",
        "result = model.transcribe(input_audio)\n",
        "\n",
        "# Save the transcription to a text file\n",
        "with open(output_text, \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(result[\"text\"])\n",
        "\n",
        "end_time = time.time()\n",
        "total_time = total_time + end_time - start_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qcw1VLQVxP6"
      },
      "source": [
        "# Text Summarization with BART\n",
        "\n",
        "This script uses the BART model from Hugging Face's Transformers library to generate a summary of a given transcription. The process includes:\n",
        "\n",
        "1. Reading the transcription text from a file.\n",
        "2. Loading the BART model for summarization (`facebook/bart-large-cnn`).\n",
        "3. Preparing the content for summarization.\n",
        "\n",
        "This script sets up the model and tokenizer, ready to summarize the transcription text into a concise version.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WL8KBj-GAQ3q"
      },
      "outputs": [],
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# Verifica se Ã¨ disponibile una GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Carica il modello BART e il tokenizer\n",
        "print(\"Loading the BART model for summarization...\")\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to(device)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Calcola il tempo totale\n",
        "total_time = end_time - start_time\n",
        "print(f\"Time taken to load the model and process the file: {total_time:.4f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IASJ4PjSWU_K"
      },
      "source": [
        "# Chunk-Based Summarization of Transcription with BART\n",
        "\n",
        "This script generates a summary for a large transcription by breaking it into smaller chunks, each of which is summarized separately. The process includes:\n",
        "\n",
        "1. Splitting the transcription text into chunks of a maximum length (1024 characters).\n",
        "2. Using the BART model to summarize each chunk with the context of previous summaries.\n",
        "3. Concatenating the individual summaries to create the final summarized text.\n",
        "4. Saving the final summary as a text file.\n",
        "\n",
        "This approach ensures that even long transcriptions can be summarized efficiently while maintaining coherence across chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGun1enwE8Ll"
      },
      "outputs": [],
      "source": [
        "# Nome del file da leggere\n",
        "file_path = \"trascription.txt\"\n",
        "# Apri e leggi il contenuto del file\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    content = file.read()\n",
        "\n",
        "transcription_text = content\n",
        "start_time = time.time()  # Start the timer to measure the total operation time.\n",
        "\n",
        "# Maximum length for each chunk in tokens\n",
        "max_length = 1024\n",
        "max_length_chunk = 900  # Maximum length for a single chunk in tokens.\n",
        "\n",
        "# Tokenize the full transcription text without truncation to calculate its total length.\n",
        "prompt_tokenized_test = tokenizer(transcription_text, return_tensors=\"pt\", truncation=False, padding=True)\n",
        "len_p = prompt_tokenized_test[\"input_ids\"].shape[1]  # Get the total number of tokens in the text.\n",
        "\n",
        "# Calculate the number of chunks based on the maximum chunk length.\n",
        "N = len_p // max_length_chunk\n",
        "text_length = len(transcription_text)\n",
        "chunk_size = text_length // N  # Calculate approximate character length per chunk.\n",
        "\n",
        "# Split the text into N equal parts.\n",
        "chunks = [transcription_text[i:i + chunk_size] for i in range(0, text_length, chunk_size)]\n",
        "\n",
        "# Ensure the last chunk contains any remaining characters.\n",
        "if len(chunks) > N:\n",
        "    chunks[-2] += chunks[-1]  # Merge the last chunk with the second last.\n",
        "    chunks = chunks[:-1]  # Remove the last (now empty) chunk.\n",
        "\n",
        "# List to store the summaries of each chunk.\n",
        "summaries = []\n",
        "\n",
        "# Process each chunk to generate summaries.\n",
        "for i, chunk in enumerate(chunks):\n",
        "    # Create a prompt for summarization including the context of the previous summary.\n",
        "    prompt = f\"\"\"\n",
        "    {chunk}\n",
        "    \"\"\"\n",
        "    # Tokenize the prompt and move it to the GPU.\n",
        "    prompt_tokenized = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=1024).to(device)\n",
        "\n",
        "    minThreshold = int((i / len(chunks)) * 1000)  # Gradually increase the minimum threshold for summary length.\n",
        "\n",
        "    # Generate the summary using the model with adjusted parameters\n",
        "    outputs = model.generate(\n",
        "        prompt_tokenized[\"input_ids\"],\n",
        "        max_length=1024,  # Maximum length for the summary\n",
        "        min_length=300,  # Increase the minimum length to ensure more detail\n",
        "        do_sample=True,  # Enable sampling for more variability in the output\n",
        "        temperature=0.2,  # Set temperature higher for more diversity in the responses\n",
        "        top_p=0.9,  # Increase top_p to consider a wider pool of tokens\n",
        "        length_penalty=1.0,  # Control length penalties to avoid very short outputs\n",
        "    )\n",
        "\n",
        "    # Decode the generated summary and append it to the summaries list.\n",
        "    new_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    summaries.append(new_summary)\n",
        "\n",
        "    # Update the context with the latest summary for the next chunk.\n",
        "\n",
        "    print(f\"Processed chunk [{i + 1}/{len(chunks)}]\")\n",
        "\n",
        "# End the timer.\n",
        "end_time = time.time()\n",
        "# Calculate the total operation time.\n",
        "total_time = end_time - start_time\n",
        "\n",
        "# Save all chunk summaries to a separate file.\n",
        "joined_output_path = \"final_summary.txt\"\n",
        "with open(joined_output_path, \"w\", encoding=\"utf-8\") as file:\n",
        "    for i, summary in enumerate(summaries, start=1):\n",
        "        file.write(summary)\n",
        "        file.write(\"\\n\")  # Add a blank line between summaries.\n",
        "    file.write(f\"The total operation is completed in: {int(total_time // 60)} minutes and {total_time % 60:.2f} seconds\")\n",
        "\n",
        "\n",
        "print(f\"Summary completed! Final summaries saved in: {joined_output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJvvbqcsAQoL"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
