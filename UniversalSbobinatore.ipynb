{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#UniversalSbobinatore"
      ],
      "metadata": {
        "id": "4Bx5rBfgTUBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation of Dependencies and Environment Setup\n",
        "In the next block, we will install the necessary dependencies and set up the environment to run the subsequent code. This includes installing libraries such as Whisper, MoviePy, Transformers, Selenium, and others, as well as configuring the Chromium browser."
      ],
      "metadata": {
        "id": "WprGY38WTXvV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuAaK2kH6SZZ"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg\n",
        "!pip install moviepy\n",
        "!pip install --upgrade transformers torch\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install selenium webdriver_manager requests\n",
        "!apt-get update -y\n",
        "!apt-get install -y chromium-browser\n",
        "!apt-get install -y chromedriver\n",
        "!apt install chromium-chromedriver"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting Audio from Video and Transcription with Whisper\n",
        "\n",
        "This script uses `moviepy` to extract audio from a video file and save it as an MP3 file, then uses OpenAI's Whisper model for potential transcription. The process includes:\n",
        "\n",
        "1. Loading the video file in MP4 format.\n",
        "2. Extracting the audio and saving it as an MP3 file.\n",
        "3. Optionally, using Whisper for audio transcription (not included in this snippet but could be added).\n",
        "   \n",
        "The result is an MP3 file that contains the audio from the video, ready for further processing or transcription.\n"
      ],
      "metadata": {
        "id": "s9qL02qCVHHa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIVno5is6xJx"
      },
      "outputs": [],
      "source": [
        "# Path to the MP4 file\n",
        "input_file = \"\" # NAME OF YOUR VIDEO\n",
        "\n",
        "\n",
        "from moviepy.editor import *\n",
        "import whisper\n",
        "\n",
        "# Load the Whisper model\n",
        "model = whisper.load_model(\"medium\")  # You can use \"tiny\", \"base\", \"small\", \"medium\", or \"large\"\n",
        "\n",
        "# Name of the output MP3 file\n",
        "output_file = \"audio.mp3\"\n",
        "\n",
        "# Load the video\n",
        "video = VideoFileClip(input_file)\n",
        "\n",
        "# Extract the audio and save it as MP3\n",
        "video.audio.write_audiofile(output_file)\n",
        "\n",
        "print(f\"Conversion completed! File saved as: {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio Transcription with Whisper\n",
        "\n",
        "This script uses OpenAI's Whisper model to transcribe audio from an MP3 file into text. The process includes:\n",
        "\n",
        "1. Loading the audio file in MP3 format.\n",
        "2. Transcribing the audio using the Whisper model.\n",
        "3. Saving the transcription as a text file.\n",
        "\n",
        "The result is a text file containing the full transcription of the audio, ready for review or further processing.\n"
      ],
      "metadata": {
        "id": "_STGCwuqVeL8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRcsGt4x6eDT"
      },
      "outputs": [],
      "source": [
        "# Path to the MP3 file\n",
        "input_audio = \"audio.mp3\"\n",
        "# Name of the output file\n",
        "output_text = \"trascription.txt\"\n",
        "\n",
        "# Transcribe the audio\n",
        "result = model.transcribe(input_audio)\n",
        "\n",
        "# Save the transcription to a text file\n",
        "with open(output_text, \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(result[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Summarization with BART\n",
        "\n",
        "This script uses the BART model from Hugging Face's Transformers library to generate a summary of a given transcription. The process includes:\n",
        "\n",
        "1. Reading the transcription text from a file.\n",
        "2. Loading the BART model for summarization (`facebook/bart-large-cnn`).\n",
        "3. Preparing the content for summarization.\n",
        "\n",
        "This script sets up the model and tokenizer, ready to summarize the transcription text into a concise version.\n"
      ],
      "metadata": {
        "id": "8qcw1VLQVxP6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WL8KBj-GAQ3q"
      },
      "outputs": [],
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "# Name of the file to read\n",
        "file_path = \"trascription.txt\"\n",
        "\n",
        "# Open and read the content of the file\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    content = file.read()\n",
        "\n",
        "transcription_text = content\n",
        "\n",
        "# Load the BART model\n",
        "print(\"Loading the BART model for summarization...\")\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunk-Based Summarization of Transcription with BART\n",
        "\n",
        "This script generates a summary for a large transcription by breaking it into smaller chunks, each of which is summarized separately. The process includes:\n",
        "\n",
        "1. Splitting the transcription text into chunks of a maximum length (1024 characters).\n",
        "2. Using the BART model to summarize each chunk with the context of previous summaries.\n",
        "3. Concatenating the individual summaries to create the final summarized text.\n",
        "4. Saving the final summary as a text file.\n",
        "\n",
        "This approach ensures that even long transcriptions can be summarized efficiently while maintaining coherence across chunks.\n"
      ],
      "metadata": {
        "id": "IASJ4PjSWU_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Maximum length for each chunk (approximation in characters)\n",
        "max_length = 1024\n",
        "\n",
        "# Split the text into chunks based on the maximum length in characters\n",
        "chunks = [transcription_text[i:i + max_length] for i in range(0, len(transcription_text), max_length)]\n",
        "# Generate the summary for each chunk\n",
        "summaries = []\n",
        "\n",
        "# Initialize an empty summary for the context of the summary\n",
        "summary = \"\"\n",
        "minThreshold=0\n",
        "for i, chunk in enumerate(chunks):\n",
        "    # Prompt for the summary with the context of the previous summary\n",
        "    prompt = f\"\"\"SUMMARIZE THIS TRANSCRIPTION OF A LECTURE OF {COURSE_TITLE} AS A UNIVERSITY STUDENT THAT IS TAKING NOTES: {summary if summary else ''}. {chunk}. RESPONSE: \"\"\"\n",
        "    print(prompt)\n",
        "    # Tokenize the prompt\n",
        "    prompt_tokenized = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    # Generate the summary using input_ids and attention_mask\n",
        "    minThreshold = (i / len(chunks)) * 600\n",
        "    outputs = model.generate(\n",
        "        prompt_tokenized[\"input_ids\"],\n",
        "        max_length=1023,\n",
        "        min_length=100+minThreshold,\n",
        "        length_penalty=2.0,\n",
        "        temperature=0.4,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "    # Decode the result\n",
        "    new_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Add the summary to the final result\n",
        "    summaries.append(new_summary)\n",
        "\n",
        "    # Update the summary for the next chunk\n",
        "    summary = new_summary\n",
        "\n",
        "print(summary)\n",
        "\n",
        "# Combine the generated summaries\n",
        "final_summary = \" \".join(summaries)\n",
        "\n",
        "# Save the summary\n",
        "output_summary = \"final_summary.txt\"\n",
        "with open(output_summary, \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(final_summary)\n",
        "\n",
        "print(f\"Summary completed! Saved in: {output_summary}\")\n"
      ],
      "metadata": {
        "id": "qGun1enwE8Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJvvbqcsAQoL"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}