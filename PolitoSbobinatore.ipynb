{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#PolitoSbobinatore"
      ],
      "metadata": {
        "id": "4Bx5rBfgTUBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation of Dependencies and Environment Setup\n",
        "In the next block, we will install the necessary dependencies and set up the environment to run the subsequent code. This includes installing libraries such as Whisper, MoviePy, Transformers, Selenium, and others, as well as configuring the Chromium browser."
      ],
      "metadata": {
        "id": "WprGY38WTXvV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuAaK2kH6SZZ"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg\n",
        "!pip install moviepy\n",
        "!pip install --upgrade transformers torch\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install selenium webdriver_manager requests\n",
        "!apt-get update -y\n",
        "!apt-get install -y chromium-browser\n",
        "!apt-get install -y chromedriver\n",
        "!apt install chromium-chromedriver"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Insert Your Polito Credentials and Course Information\n",
        "\n",
        "Before running the script, fill in your PoliTo credentials and the course details:\n"
      ],
      "metadata": {
        "id": "TqbQTe4zUFFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#INSERT HERE YOUR DATA\n",
        "USERNAME = #POLITO USERNAME\n",
        "PASSWORD = #POLITO PASSWORD\n",
        "COURSE_TITLE = #COURSE NAME\n",
        "LECTURE_TITLE = #LECTURE NAME\n"
      ],
      "metadata": {
        "id": "z3XRf9ZHOmIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automated Login and Video Download from Virtual Classroom\n",
        "\n",
        "This script uses Selenium to automate the login to the Politecnico di Torino portal, navigate through the platform, and download the lecture video from the Virtual Classroom. The process includes:\n",
        "\n",
        "1. Automatic login to the PoliTo portal with credentials.\n",
        "2. Navigation to the specified course and desired lecture.\n",
        "3. Extraction of the video URL from the lecture page.\n",
        "4. Downloading the video as an MP4 file to the local system.\n",
        "\n",
        "The browser is run in headless mode (without a graphical interface) for faster processing and without the need for manual interaction.\n"
      ],
      "metadata": {
        "id": "VO_7vIARUyXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# Settings to use Chromium in headless mode\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument('--headless')  # Headless mode\n",
        "chrome_options.add_argument('--no-sandbox')  # Necessary for running in a container environment\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')  # Necessary to avoid memory issues\n",
        "\n",
        "# Configure the Chromium driver\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# URL and credentials\n",
        "URL = \"https://idp.polito.it/home\"\n",
        "USERNAME_FIELD_ID = \"username\"\n",
        "PASSWORD_FIELD_ID = \"password\"\n",
        "LOGIN_BUTTON_CSS_SELECTOR = \"button.login.button\"\n",
        "\n",
        "# Maximize the window\n",
        "driver.maximize_window()\n",
        "\n",
        "try:\n",
        "    # Open the login page\n",
        "    driver.get(URL)\n",
        "\n",
        "    # Find the username field and enter the value\n",
        "    username_box = driver.find_element(By.ID, USERNAME_FIELD_ID)\n",
        "    username_box.send_keys(USERNAME)\n",
        "\n",
        "    # Find the password field and enter the value\n",
        "    password_box = driver.find_element(By.ID, PASSWORD_FIELD_ID)\n",
        "    password_box.send_keys(PASSWORD)\n",
        "\n",
        "    # Find and click the login button\n",
        "    login_button = driver.find_element(By.CSS_SELECTOR, LOGIN_BUTTON_CSS_SELECTOR)\n",
        "    login_button.click()\n",
        "\n",
        "    # Wait 10 seconds for the page to load\n",
        "    time.sleep(10)\n",
        "\n",
        "    # Navigate to the specified course using the course title\n",
        "    link_Course = driver.find_element(By.LINK_TEXT, COURSE_TITLE)\n",
        "    link_Course.click()\n",
        "\n",
        "    # Click the link for the \"Virtual classroom\"\n",
        "    link_virtualClass = driver.find_element(By.LINK_TEXT, \"Virtual classroom\")\n",
        "    link_virtualClass.click()\n",
        "\n",
        "    # Find and click the link for the specific lecture \"Efficient fine-tuning, inference\"\n",
        "    link_vc = driver.find_element(By.XPATH, f\"//a[text()='{LECTURE_TITLE}']\")\n",
        "    link_vc.click()\n",
        "\n",
        "    # Wait 10 seconds for the video to load\n",
        "    time.sleep(10)\n",
        "\n",
        "    # Find the video element using the class 'video-js' and get the video URL\n",
        "    video_element = driver.find_element(By.CLASS_NAME, \"video-js\")\n",
        "    video_url = video_element.find_element(By.TAG_NAME, \"source\").get_attribute(\"src\")\n",
        "\n",
        "    # Download the video using the obtained URL\n",
        "    print(f\"Video URL found: {video_url}\")\n",
        "    video_response = requests.get(video_url)\n",
        "\n",
        "    # Save the video to disk with the name \"video.mp4\"\n",
        "    with open(\"video.mp4\", \"wb\") as video_file:\n",
        "        video_file.write(video_response.content)\n",
        "    print(\"Video downloaded successfully!\")\n",
        "\n",
        "    # Print the page title for verification\n",
        "    print(\"Page title:\", driver.title)\n",
        "\n",
        "finally:\n",
        "    # Wait 10 seconds to see the result\n",
        "    time.sleep(10)\n",
        "    # Close the browser\n",
        "    driver.quit()"
      ],
      "metadata": {
        "id": "q5u_KDGjA57U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting Audio from Video and Transcription with Whisper\n",
        "\n",
        "This script uses `moviepy` to extract audio from a video file and save it as an MP3 file, then uses OpenAI's Whisper model for potential transcription. The process includes:\n",
        "\n",
        "1. Loading the video file in MP4 format.\n",
        "2. Extracting the audio and saving it as an MP3 file.\n",
        "3. Optionally, using Whisper for audio transcription (not included in this snippet but could be added).\n",
        "   \n",
        "The result is an MP3 file that contains the audio from the video, ready for further processing or transcription.\n"
      ],
      "metadata": {
        "id": "s9qL02qCVHHa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIVno5is6xJx"
      },
      "outputs": [],
      "source": [
        "from moviepy.editor import *\n",
        "import whisper\n",
        "\n",
        "# Load the Whisper model\n",
        "model = whisper.load_model(\"medium\")  # You can use \"tiny\", \"base\", \"small\", \"medium\", or \"large\"\n",
        "\n",
        "# Path to the MP4 file\n",
        "input_file = \"video.mp4\"\n",
        "# Name of the output MP3 file\n",
        "output_file = \"audio.mp3\"\n",
        "\n",
        "# Load the video\n",
        "video = VideoFileClip(input_file)\n",
        "\n",
        "# Extract the audio and save it as MP3\n",
        "video.audio.write_audiofile(output_file)\n",
        "\n",
        "print(f\"Conversion completed! File saved as: {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio Transcription with Whisper\n",
        "\n",
        "This script uses OpenAI's Whisper model to transcribe audio from an MP3 file into text. The process includes:\n",
        "\n",
        "1. Loading the audio file in MP3 format.\n",
        "2. Transcribing the audio using the Whisper model.\n",
        "3. Saving the transcription as a text file.\n",
        "\n",
        "The result is a text file containing the full transcription of the audio, ready for review or further processing.\n"
      ],
      "metadata": {
        "id": "_STGCwuqVeL8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRcsGt4x6eDT"
      },
      "outputs": [],
      "source": [
        "# Path to the MP3 file\n",
        "input_audio = \"audio.mp3\"\n",
        "# Name of the output file\n",
        "output_text = \"trascription.txt\"\n",
        "\n",
        "# Transcribe the audio\n",
        "result = model.transcribe(input_audio)\n",
        "\n",
        "# Save the transcription to a text file\n",
        "with open(output_text, \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(result[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Summarization with BART\n",
        "\n",
        "This script uses the BART model from Hugging Face's Transformers library to generate a summary of a given transcription. The process includes:\n",
        "\n",
        "1. Reading the transcription text from a file.\n",
        "2. Loading the BART model for summarization (`facebook/bart-large-cnn`).\n",
        "3. Preparing the content for summarization.\n",
        "\n",
        "This script sets up the model and tokenizer, ready to summarize the transcription text into a concise version.\n"
      ],
      "metadata": {
        "id": "8qcw1VLQVxP6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WL8KBj-GAQ3q"
      },
      "outputs": [],
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "# Name of the file to read\n",
        "file_path = \"trascription.txt\"\n",
        "\n",
        "# Open and read the content of the file\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    content = file.read()\n",
        "\n",
        "transcription_text = content\n",
        "\n",
        "# Load the BART model\n",
        "print(\"Loading the BART model for summarization...\")\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunk-Based Summarization of Transcription with BART\n",
        "\n",
        "This script generates a summary for a large transcription by breaking it into smaller chunks, each of which is summarized separately. The process includes:\n",
        "\n",
        "1. Splitting the transcription text into chunks of a maximum length (1024 characters).\n",
        "2. Using the BART model to summarize each chunk with the context of previous summaries.\n",
        "3. Concatenating the individual summaries to create the final summarized text.\n",
        "4. Saving the final summary as a text file.\n",
        "\n",
        "This approach ensures that even long transcriptions can be summarized efficiently while maintaining coherence across chunks.\n"
      ],
      "metadata": {
        "id": "IASJ4PjSWU_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Maximum length for each chunk (approximation in characters)\n",
        "max_length = 1024\n",
        "\n",
        "# Split the text into chunks based on the maximum length in characters\n",
        "chunks = [transcription_text[i:i + max_length] for i in range(0, len(transcription_text), max_length)]\n",
        "# Generate the summary for each chunk\n",
        "summaries = []\n",
        "\n",
        "# Initialize an empty summary for the context of the summary\n",
        "summary = \"\"\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    # Prompt for the summary with the context of the previous summary\n",
        "    prompt = f\"\"\"SUMMARIZE THIS TRANSCRIPTION OF A LECTURE OF {COURSE_TITLE} AS A UNIVERSITY STUDENT THAT IS TAKING NOTES: {summary if summary else ''}. {chunk}. RESPONSE: \"\"\"\n",
        "    print(prompt)\n",
        "    # Tokenize the prompt\n",
        "    prompt_tokenized = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    # Generate the summary using input_ids and attention_mask\n",
        "    outputs = model.generate(\n",
        "        prompt_tokenized[\"input_ids\"],\n",
        "        max_length=1023,\n",
        "        min_length=100,\n",
        "        length_penalty=2.0,\n",
        "        temperature=0.4,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "    # Decode the result\n",
        "    new_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Add the summary to the final result\n",
        "    summaries.append(new_summary)\n",
        "\n",
        "    # Update the summary for the next chunk\n",
        "    summary = new_summary\n",
        "\n",
        "print(summary)\n",
        "\n",
        "# Combine the generated summaries\n",
        "final_summary = \" \".join(summaries)\n",
        "\n",
        "# Save the summary\n",
        "output_summary = \"final_summary.txt\"\n",
        "with open(output_summary, \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(final_summary)\n",
        "\n",
        "print(f\"Summary completed! Saved in: {output_summary}\")\n"
      ],
      "metadata": {
        "id": "qGun1enwE8Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJvvbqcsAQoL"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}